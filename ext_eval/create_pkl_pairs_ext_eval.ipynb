{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import demoji\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.append(\"#\")\n",
    "stopwords.append(\"<unk>\")\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "                       '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]+\", tweet.lower())).strip()\n",
    "    tweet = preprocess(tweet)\n",
    "    tweet = extra_preprocess(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUsernames(string):\n",
    "    return re.sub('@[^\\s]+', '@user', string)\n",
    "\n",
    "\n",
    "def specialUnicodeRemover(string):\n",
    "    return re.sub(r\"(\\xe9|\\362)\", \"\", string)\n",
    "\n",
    "\n",
    "def punctuationRemover(tweet):\n",
    "    ls = list(string.punctuation)\n",
    "    ls.remove('@')\n",
    "    try:\n",
    "        tweet = tweet.translate(str.maketrans('', '', ls))\n",
    "        return tweet\n",
    "    except:\n",
    "        return tweet\n",
    "\n",
    "\n",
    "def RTRemover(string):\n",
    "    string = string.strip()\n",
    "    if 'RT' in string[0:2]:\n",
    "        string = string[2:]\n",
    "        return string\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "def EmojiRemover(string):\n",
    "    return demoji.replace(string, \"\")\n",
    "\n",
    "\n",
    "def DotRemover(string):\n",
    "    if '...' in string:\n",
    "        string = string.replace('...', '')\n",
    "    elif '..' in string:\n",
    "        string = string.replace('..', '')\n",
    "    return string\n",
    "\n",
    "\n",
    "def extra_preprocess(string):\n",
    "    string = removeUsernames(string)\n",
    "    string = specialUnicodeRemover(string)\n",
    "    string = punctuationRemover(string)\n",
    "    string = RTRemover(string)\n",
    "    string = EmojiRemover(string)\n",
    "    string = DotRemover(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"ext_eval/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep pair for neutral\n",
    "def f1():\n",
    "    model_file = \"neutral_455.pkl\"\n",
    "    file = os.path.join(folder, model_file)\n",
    "    with open(file, \"rb\") as f:\n",
    "        org_file = pickle.load(f)\n",
    "    # print(type(org_file))\n",
    "    ground = org_file[\"input\"]\n",
    "    pred = org_file[\"pred\"]\n",
    "    assert len(ground) == len(pred)\n",
    "    print(len(ground))\n",
    "    ground_final = []\n",
    "    pred_final = []\n",
    "    for g, p in zip(ground, pred):\n",
    "        g = basic_tokenize(g).strip()\n",
    "        p = basic_tokenize(p).strip()\n",
    "        if not g or not p:\n",
    "            continue\n",
    "        ground_final.append(g)\n",
    "        pred_final.append(p)\n",
    "    print(len(ground_final))\n",
    "\n",
    "    out = {\"ground\":ground_final,\"pred\":pred_final}\n",
    "    model_name = model_file.split(\"_\")[0]\n",
    "    file = os.path.join(folder, model_name + \"_for_ext_eval.pkl\")\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep pair for DRG\n",
    "def f2():\n",
    "    model_file = \"drgpreds_455.pkl\"\n",
    "    file = os.path.join(folder, model_file)\n",
    "    with open(file, \"rb\") as f:\n",
    "        org_file = pickle.load(f)\n",
    "    print(len(org_file))\n",
    "    file = os.path.join(folder, \"test_small_455.pkl\")\n",
    "    with open(file,\"rb\") as f:\n",
    "        test_file = pickle.load(f)\n",
    "    assert len(org_file)==len(test_file)\n",
    "    ground_final = []\n",
    "    pred_final = []\n",
    "    for g, p in zip(test_file, org_file):\n",
    "        g = basic_tokenize(g).strip()\n",
    "        p = basic_tokenize(p).strip()\n",
    "        if not g or not p:\n",
    "            continue\n",
    "        ground_final.append(g)\n",
    "        pred_final.append(p)\n",
    "    print(len(ground_final))\n",
    "\n",
    "    out = {\"ground\":ground_final,\"pred\":pred_final}\n",
    "    model_name = model_file.split(\"_\")[0]\n",
    "    file = os.path.join(folder, model_name + \"_for_ext_eval.pkl\")\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep pair for NTP\n",
    "def f3():\n",
    "    model_file = \"ntpcares_454.pkl\"\n",
    "    file = os.path.join(folder, model_file)\n",
    "    with open(file, \"rb\") as f:\n",
    "        org_file = pickle.load(f)\n",
    "    print(len(org_file))\n",
    "    file = os.path.join(folder, \"test_dev_454.pkl\")\n",
    "    with open(file,\"rb\") as f:\n",
    "        test_file = pickle.load(f)\n",
    "    assert len(org_file)==len(test_file)\n",
    "    ground_final = []\n",
    "    pred_final = []\n",
    "    for g, p in zip(test_file, org_file):\n",
    "        g = basic_tokenize(g).strip()\n",
    "        p = basic_tokenize(p).strip()\n",
    "        if not g or not p:\n",
    "            continue\n",
    "        ground_final.append(g)\n",
    "        pred_final.append(p)\n",
    "    print(len(ground_final))\n",
    "\n",
    "    out = {\"ground\":ground_final,\"pred\":pred_final}\n",
    "    model_name = model_file.split(\"_\")[0]\n",
    "    file = os.path.join(folder, model_name + \"_for_ext_eval.pkl\")\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep pair for FGST\n",
    "def f4():\n",
    "    model_file = \"fgst_910.pkl\"\n",
    "    file = os.path.join(folder, model_file)\n",
    "    with open(file, \"rb\") as f:\n",
    "        org_file = pickle.load(f)\n",
    "    print(len(org_file))\n",
    "    print(org_file.keys())\n",
    "    ground = org_file[\"input\"]\n",
    "    pred1 = org_file[1]\n",
    "    pred2 = org_file[2]\n",
    "    pred3 = org_file[3]\n",
    "    pred4 = org_file[4]\n",
    "    pred5 = org_file[5]\n",
    "    gr1=[]\n",
    "    gr2=[]\n",
    "    gr3=[]\n",
    "    gr4=[]\n",
    "    gr5=[]\n",
    "    pr1=[]\n",
    "    pr2=[]\n",
    "    pr3=[]\n",
    "    pr4=[]\n",
    "    pr5=[]\n",
    "    for g,p1,p2,p3,p4,p5 in zip(ground,pred1,pred2,pred3,pred4,pred5):\n",
    "        g = basic_tokenize(g).strip()\n",
    "        p1 = basic_tokenize(p1).strip()\n",
    "        p2 = basic_tokenize(p2).strip()\n",
    "        p3 = basic_tokenize(p3).strip()\n",
    "        p4 = basic_tokenize(p4).strip()\n",
    "        p5 = basic_tokenize(p5).strip()\n",
    "        if not g:\n",
    "            continue\n",
    "        if p1:  \n",
    "            gr1.append(g)\n",
    "            pr1.append(p1)\n",
    "        if p2:  \n",
    "            gr2.append(g)\n",
    "            pr2.append(p2)\n",
    "        if p3:  \n",
    "            gr3.append(g)\n",
    "            pr3.append(p3)\n",
    "        if p4:  \n",
    "            gr4.append(g)\n",
    "            pr4.append(p4)\n",
    "        if p5:  \n",
    "            gr5.append(g)\n",
    "            pr5.append(p5)\n",
    "    assert len(gr1)==len(pr1)\n",
    "    assert len(gr2)==len(pr2)\n",
    "    assert len(gr3)==len(pr3)\n",
    "    assert len(gr4)==len(pr4)\n",
    "    assert len(gr5)==len(pr5)\n",
    "    print(len(gr1))\n",
    "    print(len(gr2))\n",
    "    print(len(gr3))\n",
    "    print(len(gr4))\n",
    "    print(len(gr5))\n",
    "    out1 = {\"ground\":gr1,\"pred\":pr1}\n",
    "    out2 = {\"ground\":gr2,\"pred\":pr2}\n",
    "    out3 = {\"ground\":gr3,\"pred\":pr3}\n",
    "    out4 = {\"ground\":gr4,\"pred\":pr4}\n",
    "    out5 = {\"ground\":gr5,\"pred\":pr5}\n",
    "    out = {1:out1,2:out2,3:out3,4:out4,5:out5}\n",
    "\n",
    "    model_name = model_file.split(\"_\")[0]\n",
    "    file = os.path.join(folder, model_name + \"_for_ext_eval.pkl\")\n",
    "    with open(file, \"wb\") as f:\n",
    "            pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare pairs for Style\n",
    "def f5():\n",
    "    model_file = \"style_910.pkl\"\n",
    "    file = os.path.join(folder, model_file)\n",
    "    with open(file, \"rb\") as f:\n",
    "        org_file = pickle.load(f)\n",
    "    print(org_file.keys())\n",
    "    test = org_file['input']\n",
    "    pred_rev = org_file['pred_rev']\n",
    "    pred_raw = org_file['pred_raw']\n",
    "    assert len(test)==len(pred_rev)==len(pred_raw)\n",
    "    print(len(test))\n",
    "    gr1=[]\n",
    "    gr2=[]\n",
    "    pr1=[]\n",
    "    pr2=[]\n",
    "    for g,p1,p2 in zip(test,pred_rev,pred_raw):\n",
    "        g = basic_tokenize(g).strip()\n",
    "        p1 = basic_tokenize(p1).strip()\n",
    "        p2 = basic_tokenize(p2).strip()\n",
    "        if not g:\n",
    "            continue\n",
    "        if p1:  \n",
    "            gr1.append(g)\n",
    "            pr1.append(p1)\n",
    "        if p2:  \n",
    "            gr2.append(g)\n",
    "            pr2.append(p2)\n",
    "    assert len(gr1)==len(pr1)\n",
    "    assert len(gr2)==len(pr2)\n",
    "    print(len(gr1))\n",
    "    print(len(gr2))\n",
    "    out1 = {\"ground\":gr1,\"pred\":pr1}\n",
    "    out2 = {\"ground\":gr2,\"pred\":pr2}\n",
    "    out = {1:out1,2:out2}\n",
    "\n",
    "    model_name = model_file.split(\"_\")[0]\n",
    "    file = os.path.join(folder, model_name + \"_for_ext_eval.pkl\")\n",
    "    with open(file, \"wb\") as f:\n",
    "            pickle.dump(out, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep pairs for NACL\n",
    "def f6():\n",
    "    ground_final = []\n",
    "    pred_final = []\n",
    "    model_file = \"nacl_pred.csv\"\n",
    "    file = os.path.join(folder, model_file)\n",
    "    df = pd.read_csv(file)\n",
    "    for g,p in zip(df['Original_text'],df['Pred1']):\n",
    "        if g==\"Next Cross Validation\":\n",
    "            continue\n",
    "        g = g[2:-2]\n",
    "        p = p[2:-2]\n",
    "        g = basic_tokenize(g).strip()\n",
    "        p = basic_tokenize(p).strip()\n",
    "        if g and p:\n",
    "            ground_final.append(g)\n",
    "            pred_final.append(p)\n",
    "    assert len(ground_final)==len(pred_final)\n",
    "    print(len(ground_final))\n",
    "    out = {\"ground\":ground_final,\"pred\":pred_final}\n",
    "    model_name = model_file.split(\"_\")[0]\n",
    "    file = os.path.join(folder, model_name + \"_for_ext_eval.pkl\")\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_():\n",
    "    f1()\n",
    "    f2()\n",
    "    f3()\n",
    "    f4()\n",
    "    f5()\n",
    "    f6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
