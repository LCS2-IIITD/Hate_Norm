{"cells":[{"cell_type":"markdown","metadata":{"id":"Z_0DfodnVlef"},"source":["## Hate Intensity Reduction (HIP): Generation\n","\n","Given a hateful sentence generate/paraphase it to in form of a normlised sentence. This normalised sentence is then rewarded with a reduction in intensity."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrNz5LASVlek","executionInfo":{"status":"ok","timestamp":1659513294589,"user_tz":-330,"elapsed":7313,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}},"outputId":"3d786eac-80c0-4804-ecbb-f4394b473a8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.0\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"]},{"output_type":"stream","name":"stdout","text":["3.4.0\n","1.19.5\n"]}],"source":["# !pip install numpy==1.19.5\n","# !pip install tensorflow==2.2.0\n","# !pip install transformers==3.4.0\n","# !pip install tensorflow-addons==0.10.0\n","# !pip install torch==1.4.0\n","# !pip install simpletransformers==0.49.0\n","# !pip install sklearn scipy\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","import transformers\n","import simpletransformers\n","print(transformers.__version__)\n","\n","import numpy as np\n","print(np.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBljwhOBVlej","executionInfo":{"status":"ok","timestamp":1659513331917,"user_tz":-330,"elapsed":21819,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}},"outputId":"fb8b25b2-f907-4868-9fbd-bebe7cfed11a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## The folder is setup to from google drive. If used else only the following lines needs commenting\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hkIZvMYKVlel","executionInfo":{"status":"ok","timestamp":1659513337102,"user_tz":-330,"elapsed":404,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}}},"outputs":[],"source":["import random\n","import pickle \n","import numpy as np\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import BertTokenizer\n","from transformers import DistilBertTokenizer, RobertaTokenizer, BertConfig, TFBertModel\n","import random\n","import sys \n","import math\n","\n","from simpletransformers.config.global_args import global_args\n","from simpletransformers.config.model_args import Seq2SeqArgs\n","from simpletransformers.seq2seq.seq2seq_utils import Seq2SeqDataset, SimpleSummarizationDataset\n","\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.model_selection import KFold\n","from tensorflow.keras.models import Sequential, load_model\n","from sklearn.metrics import mean_squared_error\n","\n","import os\n","\n","import logging\n","import os\n","import pickle\n","from multiprocessing import Pool\n","from typing import Tuple\n","\n","import pandas as pd\n","import torch\n","from tokenizers.implementations import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","from torch.utils.data import Dataset\n","from tqdm.auto import tqdm\n","from transformers import PreTrainedTokenizer\n","\n","import json\n","import logging\n","import math\n","import os\n","import random\n","import warnings\n","from dataclasses import asdict\n","from multiprocessing import Pool, cpu_count\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.auto import tqdm, trange"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"P3bwiDbtVlem","executionInfo":{"status":"ok","timestamp":1659513343914,"user_tz":-330,"elapsed":405,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}}},"outputs":[],"source":["BASE_FOLDER = \"/content/drive/MyDrive/hate_norm_kdd22/\"\n","OUTPUT_FOLDER = \"BART_GEN\"\n","OUTPUT_FILE = \"BART_GEN_FILE\"\n","INPUT_HT_FOLDER=\"hate_intensity_linear_weights_att/\"\n","INPUT_HT_FILE=\"hate_int_linear_trans42_ATT\"\n","BERT_MODEL = \"distilbert-base-uncased\"\n","BART_INPUT = \"train_df.csv\"\n","MAX_LENGTH = 128\n","SEED = 42\n","\n","USE_ATT = True\n","\n","BERT_DROPOUT = 0.2\n","LSTM_UNITS = 50\n","DENSE_UNITS = 50\n","LSTM_DROPOUT = 0.1\n","DENSE_DROPOUT = 0.2\n","\n","\n","def random_seed(SEED):\n","    random.seed(SEED)\n","    os.environ['PYTHONHASHSEED'] = str(SEED)\n","    np.random.seed(SEED)\n","    tf.random.set_seed(SEED)\n","\n","random_seed(SEED)"]},{"cell_type":"markdown","metadata":{"id":"Z-RyXCHMVlen"},"source":["### Load the intensity module"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":882,"referenced_widgets":["bb9dfc3e6a7b48daa91c5fbb46a0fd9f","b7be617edc034c1285104c5a7deebcbc","33ba4a378cca4b00a412fcfd9460291f","a35568ad47aa42ff9273b98664c15556","a83747dcddc347349cf7cb83c4831528","6513065e7e9f4ad4be0837928a5208b0","225fbc26c7684e6eb898ac74edd0687d","e66aa8eaa920463c90369000fbb7b71d","84abd224d96e4971aacd239110e92606","b8492f202e8f433e9462c523c5720996","b164a10f2e5a40b4b0a19a10e4dd4f52","6fb0b06f0a1d4492afe5fe3133b763f6","cfd00e805b504330b5e391d96fab9e98","a127858d3e8e4cb2a270700125749799","714a3a08456c45988c048daf8e27d4c7","1dbfd5c860f54d4ab03e2631223dab21","4079b20f61c54689adfb758796c99e63","91db461ea79a4dd7b9965d82504b5844","f64a6ad84cc24d03a33cfeadf011c13b","2ac8209000b34ee9b44aa9fc975416b7","ab80eeaaa9e544cc8fa3fa83bc7b5346","7ce56e0ef85d4c248d3236e7a77b9e10"]},"id":"g9qBQ8YRVlen","executionInfo":{"status":"ok","timestamp":1659513399232,"user_tz":-330,"elapsed":52129,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}},"outputId":"3d376fdd-1218-4fb6-cc81-70a67f2045fc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9dfc3e6a7b48daa91c5fbb46a0fd9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/363M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb0b06f0a1d4492afe5fe3133b763f6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2022-08-03 07:56:20.904693: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2022-08-03 07:56:20.920439: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2022-08-03 07:56:20.920534: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b5fa7e37f2cf): /proc/driver/nvidia/version does not exist\n","2022-08-03 07:56:20.925928: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2022-08-03 07:56:20.956422: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199995000 Hz\n","2022-08-03 07:56:20.966271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7d2d100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2022-08-03 07:56:20.966341: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2022-08-03 07:56:21.020843: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n","2022-08-03 07:56:21.927625: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n","2022-08-03 07:56:22.025273: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n","Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['vocab_projector', 'distilbert', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_token (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","masked_token (InputLayer)       [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model (TFBertModel)     ((None, 128, 768), ( 109482240   input_token[0][0]                \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 128, 100)     327600      tf_bert_model[0][0]              \n","__________________________________________________________________________________________________\n","attention (Attention)           (None, 128, 100)     1           bidirectional[0][0]              \n","                                                                 bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 100)          0           attention[0][0]                  \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 50)           5050        global_max_pooling1d[0][0]       \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 50)           0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1)            51          dropout_37[0][0]                 \n","==================================================================================================\n","Total params: 109,814,942\n","Trainable params: 332,702\n","Non-trainable params: 109,482,240\n","__________________________________________________________________________________________________\n"]},{"output_type":"stream","name":"stderr","text":["2022-08-03 07:56:51.622508: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n","2022-08-03 07:57:00.704159: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n"]},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9d20d3f790>"]},"metadata":{},"execution_count":5}],"source":["tokenizer_intensity = DistilBertTokenizer.from_pretrained(\n","    BERT_MODEL,\n","    do_lower_case=True,\n","    add_special_tokens=True,\n","    max_length=MAX_LENGTH,\n","    pad_to_max_length=True)\n","\n","\n","def tokenize_bert(sentences, tokenizer=tokenizer_intensity):\n","    input_ids, input_masks, input_segments = [], [], []\n","    for sentence in tqdm(sentences):\n","        inputs = tokenizer_intensity.encode_plus(sentence,\n","                                                 add_special_tokens=True,\n","                                                 max_length=MAX_LENGTH,\n","                                                 pad_to_max_length=True,\n","                                                 return_attention_mask=True,\n","                                                 return_token_type_ids=True)\n","        # del inputs[\"special_tokens_mask\"]\n","        input_ids.append(inputs['input_ids'])\n","        input_masks.append(inputs['attention_mask'])\n","        input_segments.append(inputs['token_type_ids'])\n","    return np.asarray(input_ids, dtype='int32'), np.asarray(\n","        input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n","\n","\n","config = BertConfig(dropout=BERT_DROPOUT,\n","                    attention_dropout=BERT_DROPOUT,\n","                    output_attentions=True)\n","config.output_hidden_states = False\n","transformer_model = TFBertModel.from_pretrained(BERT_MODEL, config=config)\n","for layer in transformer_model.layers[:3]:\n","    layer.trainable = False\n","\n","input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n","                                     name='input_token',\n","                                     dtype='int32')\n","input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH, ),\n","                                       name='masked_token',\n","                                       dtype='int32')\n","embedding_layer = transformer_model(input_ids_in,\n","                                    attention_mask=input_masks_in)[0]\n","X = tf.keras.layers.Bidirectional(\n","    tf.keras.layers.LSTM(LSTM_UNITS,\n","                         return_sequences=True,\n","                         dropout=LSTM_DROPOUT,\n","                         recurrent_dropout=LSTM_DROPOUT,\n","                         kernel_initializer='normal'))(embedding_layer)\n","if USE_ATT:\n","    X = tf.keras.layers.Attention(use_scale=True)([X, X])  # Use attention.\n","X = tf.keras.layers.GlobalMaxPool1D()(X)\n","X = tf.keras.layers.Dense(DENSE_UNITS,\n","                          activation='relu',\n","                          kernel_initializer='normal')(X)\n","X = tf.keras.layers.Dropout(DENSE_DROPOUT)(X)\n","X = tf.keras.layers.Dense(1, activation='linear',\n","                          kernel_initializer='normal')(X)\n","intensity_model = tf.keras.Model(inputs=[input_ids_in, input_masks_in],\n","                                 outputs=X)\n","intensity_model.compile(\n","    optimizer='adam',\n","    loss='mean_squared_error',\n","    metrics=['acc', tf.keras.metrics.RootMeanSquaredError()])\n","intensity_model.summary()\n","intensity_model.load_weights(BASE_FOLDER + INPUT_HT_FOLDER + INPUT_HT_FILE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXsk056_Vleo"},"outputs":[],"source":["## We copy the origina code of Seq2seq model so as to form a change in its loss function"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Vg-N0DxlVlep","executionInfo":{"status":"ok","timestamp":1659513399234,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}}},"outputs":[],"source":["from transformers import (\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    BartConfig,\n","    BartForConditionalGeneration,\n","    BartTokenizer,\n","    BertConfig,\n","    BertForMaskedLM,\n","    BertModel,\n","    BertTokenizer,\n","    CamembertConfig,\n","    CamembertModel,\n","    CamembertTokenizer,\n","    DistilBertConfig,\n","    DistilBertModel,\n","    DistilBertTokenizer,\n","    ElectraConfig,\n","    ElectraModel,\n","    ElectraTokenizer,\n","    EncoderDecoderConfig,\n","    EncoderDecoderModel,\n","    LongformerConfig,\n","    LongformerModel,\n","    LongformerTokenizer,\n","    MarianConfig,\n","    MarianMTModel,\n","    MarianTokenizer,\n","    MobileBertConfig,\n","    MobileBertModel,\n","    MobileBertTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    RobertaConfig,\n","    RobertaModel,\n","    RobertaTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","kfold = KFold(n_splits=2)\n","\n","MODEL_CLASSES = {\n","    \"auto\": (AutoConfig, AutoModel, AutoTokenizer),\n","    \"bart\": (BartConfig, BartForConditionalGeneration, BartTokenizer),\n","    \"bert\": (BertConfig, BertModel, BertTokenizer),\n","    \"camembert\": (CamembertConfig, CamembertModel, CamembertTokenizer),\n","    \"distilbert\": (DistilBertConfig, DistilBertModel, DistilBertTokenizer),\n","    \"electra\": (ElectraConfig, ElectraModel, ElectraTokenizer),\n","    \"longformer\": (LongformerConfig, LongformerModel, LongformerTokenizer),\n","    \"mobilebert\": (MobileBertConfig, MobileBertModel, MobileBertTokenizer),\n","    \"marian\": (MarianConfig, MarianMTModel, MarianTokenizer),\n","    \"roberta\": (RobertaConfig, RobertaModel, RobertaTokenizer),\n","}"]},{"cell_type":"code","source":["import csv\n","output = open('/content/drive/MyDrive/hate_norm_kdd22/pred_final_new7.csv',mode='w')\n","output_writer = csv.writer(output, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","\n","heading=['Original_text','Truth','Pred1']\n","output_writer.writerow(heading)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yD-pkpJn3EG4","executionInfo":{"status":"ok","timestamp":1659513400212,"user_tz":-330,"elapsed":993,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}},"outputId":"6fa5aef9-0ca7-4cfb-809e-45e8ffc64b9d"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7eP5Y--Vleq"},"outputs":[],"source":["## In order to load run the "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"kFUsFhHoXIDY","executionInfo":{"status":"ok","timestamp":1659513746274,"user_tz":-330,"elapsed":989,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}}},"outputs":[],"source":["class Seq2SeqModel:\n","    def __init__(\n","        self,\n","        encoder_type=None,\n","        encoder_name=None,\n","        decoder_name=None,\n","        encoder_decoder_type=None,\n","        encoder_decoder_name=None,\n","        config=None,\n","        args=None,\n","        use_cuda=True,\n","        cuda_device=-1,\n","        **kwargs,\n","    ):\n","\n","        \"\"\"\n","        Initializes a Seq2SeqModel.\n","        Args:\n","            encoder_type (optional): The type of model to use as the encoder.\n","            encoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n","            decoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n","                                    Must be the same \"size\" as the encoder model (base/base, large/large, etc.)\n","            encoder_decoder_type (optional): The type of encoder-decoder model. (E.g. bart)\n","            encoder_decoder_name (optional): The path to a directory containing the saved encoder and decoder of a Seq2SeqModel. (E.g. \"outputs/\") OR a valid BART or MarianMT model.\n","            config (optional): A configuration file to build an EncoderDecoderModel.\n","            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n","            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n","            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n","            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n","        \"\"\"  # noqa: ignore flake8\"\n","\n","        if not config:\n","            # if not ((encoder_name and decoder_name) or encoder_decoder_name) and not encoder_type:\n","            if not ((encoder_name and decoder_name) or encoder_decoder_name):\n","                raise ValueError(\n","                    \"You must specify a Seq2Seq config \\t OR \\t\"\n","                    \"encoder_type, encoder_name, and decoder_name OR \\t \\t\"\n","                    \"encoder_type and encoder_decoder_name\")\n","            elif not (encoder_type or encoder_decoder_type):\n","                raise ValueError(\n","                    \"You must specify a Seq2Seq config \\t OR \\t\"\n","                    \"encoder_type, encoder_name, and decoder_name \\t OR \\t\"\n","                    \"encoder_type and encoder_decoder_name\")\n","\n","        self.args = self._load_model_args(encoder_decoder_name)\n","\n","        if isinstance(args, dict):\n","            self.args.update_from_dict(args)\n","        elif isinstance(args, Seq2SeqArgs):\n","            self.args = args\n","\n","        if \"sweep_config\" in kwargs:\n","            sweep_config = kwargs.pop(\"sweep_config\")\n","            sweep_values = {\n","                key: value[\"value\"]\n","                for key, value in sweep_config.as_dict().items()\n","                if key != \"_wandb\"\n","            }\n","            self.args.update_from_dict(sweep_values)\n","\n","        if self.args.manual_seed:\n","            random.seed(self.args.manual_seed)\n","            np.random.seed(self.args.manual_seed)\n","            torch.manual_seed(self.args.manual_seed)\n","            if self.args.n_gpu > 0:\n","                torch.cuda.manual_seed_all(self.args.manual_seed)\n","\n","        if use_cuda:\n","            if torch.cuda.is_available():\n","                if cuda_device == -1:\n","                    self.device = torch.device(\"cuda\")\n","                else:\n","                    self.device = torch.device(f\"cuda:{cuda_device}\")\n","            else:\n","                raise ValueError(\n","                    \"'use_cuda' set to True when cuda is unavailable.\"\n","                    \"Make sure CUDA is available or set `use_cuda=False`.\")\n","        else:\n","            self.device = \"cpu\"\n","\n","        self.results = {}\n","\n","        if not use_cuda:\n","            self.args.fp16 = False\n","\n","        # config = EncoderDecoderConfig.from_encoder_decoder_configs(config, config)\n","        if encoder_decoder_type:\n","            config_class, model_class, tokenizer_class = MODEL_CLASSES[\n","                encoder_decoder_type]\n","        else:\n","            config_class, model_class, tokenizer_class = MODEL_CLASSES[\n","                encoder_type]\n","\n","        if encoder_decoder_type in [\"bart\", \"marian\"]:\n","            self.model = model_class.from_pretrained(encoder_decoder_name)\n","            if encoder_decoder_type == \"bart\":\n","                self.encoder_tokenizer = tokenizer_class.from_pretrained(\n","                    encoder_decoder_name)\n","            elif encoder_decoder_type == \"marian\":\n","                if self.args.base_marian_model_name:\n","                    self.encoder_tokenizer = tokenizer_class.from_pretrained(\n","                        self.args.base_marian_model_name)\n","                else:\n","                    self.encoder_tokenizer = tokenizer_class.from_pretrained(\n","                        encoder_decoder_name)\n","            self.decoder_tokenizer = self.encoder_tokenizer\n","            self.config = self.model.config\n","        else:\n","            if encoder_decoder_name:\n","                # self.model = EncoderDecoderModel.from_pretrained(encoder_decoder_name)\n","                self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n","                    os.path.join(encoder_decoder_name, \"encoder\"),\n","                    os.path.join(encoder_decoder_name, \"decoder\"))\n","                self.model.encoder = model_class.from_pretrained(\n","                    os.path.join(encoder_decoder_name, \"encoder\"))\n","                self.model.decoder = BertForMaskedLM.from_pretrained(\n","                    os.path.join(encoder_decoder_name, \"decoder\"))\n","                self.encoder_tokenizer = tokenizer_class.from_pretrained(\n","                    os.path.join(encoder_decoder_name, \"encoder\"))\n","                self.decoder_tokenizer = BertTokenizer.from_pretrained(\n","                    os.path.join(encoder_decoder_name, \"decoder\"))\n","            else:\n","                self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n","                    encoder_name, decoder_name, config=config)\n","                self.encoder_tokenizer = tokenizer_class.from_pretrained(\n","                    encoder_name)\n","                self.decoder_tokenizer = BertTokenizer.from_pretrained(\n","                    decoder_name)\n","            self.encoder_config = self.model.config.encoder\n","            self.decoder_config = self.model.config.decoder\n","\n","        # if self.args.wandb_project and not wandb_available:\n","        #     warnings.warn(\"wandb_project specified but wandb is not available. Wandb disabled.\")\n","        #     self.args.wandb_project = None\n","\n","        if encoder_decoder_name:\n","            self.args.model_name = encoder_decoder_name\n","\n","            # # Checking if we are loading from a saved model or using a pre-trained model\n","            # if not saved_model_args and encoder_decoder_type == \"marian\":\n","            # Need to store base pre-trained model name to get the tokenizer when loading a saved model\n","            self.args.base_marian_model_name = encoder_decoder_name\n","\n","        elif encoder_name and decoder_name:\n","            self.args.model_name = encoder_name + \"-\" + decoder_name\n","        else:\n","            self.args.model_name = \"encoder-decoder\"\n","\n","        if encoder_decoder_type:\n","            self.args.model_type = encoder_decoder_type\n","        elif encoder_type:\n","            self.args.model_type = encoder_type + \"-bert\"\n","        else:\n","            self.args.model_type = \"encoder-decoder\"\n","\n","    def train_model(\n","        self,\n","        train_data,\n","        model_lr,\n","        tokenizer_new,\n","        labels,\n","        test_data,\n","        output_dir=None,\n","        show_running_loss=True,\n","        args=None,\n","        eval_data=None,\n","        verbose=True,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Trains the model using 'train_data'\n","        Args:\n","            train_data: Pandas DataFrame containing the 2 columns - `input_text`, `target_text`.\n","                        - `input_text`: The input text sequence.\n","                        - `target_text`: The target text sequence\n","            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n","            show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n","            args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n","            eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n","            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n","                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n","                        will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.\n","        Returns:\n","            None\n","        \"\"\"  # noqa: ignore flake8\"\n","        print(\"in train_model**** eval_data\", eval_data)\n","        if args:\n","            self.args.update_from_dict(args)\n","\n","        # if self.args.silent:\n","        #     show_running_loss = False\n","        '''if self.args.evaluate_during_training and eval_data is None:\n","            raise ValueError(\n","                \"evaluate_during_training is enabled but eval_data is not specified.\"\n","                \" Pass eval_data to model.train_model() if using evaluate_during_training.\"\n","            )'''\n","\n","        if not output_dir:\n","            output_dir = self.args.output_dir\n","\n","        if os.path.exists(output_dir) and os.listdir(\n","                output_dir) and not self.args.overwrite_output_dir:\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty.\"\n","                \" Set args.overwrite_output_dir = True to overcome.\".format(\n","                    output_dir))\n","\n","        self._move_model_to_device()\n","        os.makedirs(output_dir, exist_ok=True)\n","        print(\"train\")\n","        print(len(train_data))\n","        print(train_data)\n","        print(\"labels: \", labels)\n","        print(\"type: \", type(labels))\n","        print(\"Beginiing 1st kfold\")\n","        train_dataset = self.load_and_cache_examples(train_data,\n","                                                     verbose=verbose)\n","        # tr_labels=labels['id'].to_list()\n","        tr_labels = labels.tolist()\n","        print(type(tr_labels))\n","        # global_step, tr_loss = self.train(train_dataset,output_dir,model_lr,tr_labels,tokenizer_new,show_running_loss=show_running_loss,eval_data=train_df.iloc[eval_index],verbose=verbose,**kwargs,)\n","\n","        # IMPORTANT -------------------------------- FIX EVAL INDEX ^^\n","        global_step, tr_loss = self.train(\n","            train_dataset,\n","            output_dir,\n","            model_lr,\n","            tr_labels,\n","            tokenizer_new,\n","            show_running_loss=show_running_loss,\n","            eval_data=eval_data,\n","            verbose=verbose,\n","            **kwargs,\n","        )\n","        # self._save_model(self.args.output_dir, model=self.model)\n","        # output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n","        # with open(output_eval_file, \"w\") as writer:\n","        #   for key in sorted(results.keys()):\n","        #     writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","        to_predict = [\n","            str(input_text) for input_text in test_df[\"input_text\"].tolist()\n","        ]\n","        preds = model.predict(to_predict)\n","        # pred = model(to_predict)\n","        wr = []\n","        c = \"  \"\n","        w = \"Crosfold begin\"\n","        # output_writer = open(\"output.txt\",\"wb\")\n","        for i, text in enumerate(test_df[\"input_text\"].tolist()):\n","            row = []\n","            row.append(str(text))\n","            row.append(test_df[\"target_text\"][i])\n","            wr.append(w)\n","            wr.append(c)\n","            row.append(str(preds[i]))\n","            output_writer.writerow(row)\n","        output_writer.writerow(wr)\n","\n","        # model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n","        # model_to_save.save_pretrained(output_dir)\n","        # self.encoder_tokenizer.save_pretrained(output_dir)\n","        # self.decoder_tokenizer.save_pretrained(output_dir)\n","        # torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n","\n","        if verbose:\n","            logger.info(\" Training of {} model complete. Saved to {}.\".format(\n","                self.args.model_name, output_dir))\n","\n","    def train(\n","        self,\n","        train_dataset,\n","        output_dir,\n","        model_lr,\n","        tr_labels,\n","        tokenizer_new,\n","        show_running_loss=True,\n","        eval_data=None,\n","        verbose=True,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Trains the model on train_dataset.\n","        Utility function to be used by the train_model() method. Not intended to be used directly.\n","        \"\"\"\n","        print(\"ln\", len(tr_labels))\n","        # model = self.model\n","        args = self.args\n","\n","        # tb_writer = SummaryWriter(logdir=args.tensorboard_dir)\n","\n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            batch_size=args.train_batch_size,\n","            num_workers=self.args.dataloader_num_workers,\n","        )\n","        train_dataloader2 = DataLoader(\n","            tr_labels,\n","            batch_size=args.train_batch_size,\n","            num_workers=self.args.dataloader_num_workers,\n","        )\n","        print(\"1 done\")\n","\n","        if args.max_steps > 0:\n","            t_total = args.max_steps\n","            args.num_train_epochs = args.max_steps // (\n","                len(train_dataloader) // args.gradient_accumulation_steps) + 1\n","        else:\n","            t_total = len(\n","                train_dataloader\n","            ) // args.gradient_accumulation_steps * args.num_train_epochs\n","\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","        optimizer_grouped_parameters = []\n","        custom_parameter_names = set()\n","        for group in self.args.custom_parameter_groups:\n","            params = group.pop(\"params\")\n","            custom_parameter_names.update(params)\n","            param_group = {**group}\n","            param_group[\"params\"] = [\n","                p for n, p in self.model.named_parameters() if n in params\n","            ]\n","            optimizer_grouped_parameters.append(param_group)\n","\n","        for group in self.args.custom_layer_parameters:\n","            layer_number = group.pop(\"layer\")\n","            layer = f\"layer.{layer_number}.\"\n","            group_d = {**group}\n","            group_nd = {**group}\n","            group_nd[\"weight_decay\"] = 0.0\n","            params_d = []\n","            params_nd = []\n","            for n, p in self.model.named_parameters():\n","                if n not in custom_parameter_names and layer in n:\n","                    if any(nd in n for nd in no_decay):\n","                        params_nd.append(p)\n","                    else:\n","                        params_d.append(p)\n","                    custom_parameter_names.add(n)\n","            group_d[\"params\"] = params_d\n","            group_nd[\"params\"] = params_nd\n","\n","            optimizer_grouped_parameters.append(group_d)\n","            optimizer_grouped_parameters.append(group_nd)\n","\n","        if not self.args.train_custom_parameters_only:\n","            optimizer_grouped_parameters.extend([\n","                {\n","                    \"params\": [\n","                        p for n, p in self.model.named_parameters()\n","                        if n not in custom_parameter_names and not any(\n","                            nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\":\n","                    args.weight_decay,\n","                },\n","                {\n","                    \"params\": [\n","                        p for n, p in self.model.named_parameters()\n","                        if n not in custom_parameter_names and any(\n","                            nd in n for nd in no_decay)\n","                    ],\n","                    \"weight_decay\":\n","                    0.0,\n","                },\n","            ])\n","\n","        warmup_steps = math.ceil(t_total * args.warmup_ratio)\n","        args.warmup_steps = warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n","\n","        # TODO: Use custom optimizer like with BertSum?\n","        optimizer = AdamW(optimizer_grouped_parameters,\n","                          lr=args.learning_rate,\n","                          eps=args.adam_epsilon)\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=args.warmup_steps,\n","            num_training_steps=t_total)\n","\n","        if (args.model_name and os.path.isfile(\n","                os.path.join(args.model_name, \"optimizer.pt\"))\n","                and os.path.isfile(\n","                    os.path.join(args.model_name, \"scheduler.pt\"))):\n","            # Load in optimizer and scheduler states\n","            optimizer.load_state_dict(\n","                torch.load(os.path.join(args.model_name, \"optimizer.pt\")))\n","            scheduler.load_state_dict(\n","                torch.load(os.path.join(args.model_name, \"scheduler.pt\")))\n","\n","        if args.n_gpu > 1:\n","            self.model = torch.nn.DataParallel(model)\n","\n","        logger.info(\" Training started\")\n","\n","        global_step = 0\n","        tr_loss, logging_loss = 0.0, 0.0\n","        self.model.zero_grad()\n","        train_iterator = trange(int(args.num_train_epochs),\n","                                desc=\"Epoch\",\n","                                disable=args.silent,\n","                                mininterval=0)\n","        epoch_number = 0\n","        best_eval_metric = None\n","        early_stopping_counter = 0\n","        steps_trained_in_current_epoch = 0\n","        epochs_trained = 0\n","\n","        if args.model_name and os.path.exists(args.model_name):\n","            try:\n","                # set global_step to gobal_step of last saved checkpoint from model path\n","                checkpoint_suffix = args.model_name.split(\"/\")[-1].split(\"-\")\n","                if len(checkpoint_suffix) > 2:\n","                    checkpoint_suffix = checkpoint_suffix[1]\n","                else:\n","                    checkpoint_suffix = checkpoint_suffix[-1]\n","                global_step = int(checkpoint_suffix)\n","                epochs_trained = global_step // (\n","                    len(train_dataloader) // args.gradient_accumulation_steps)\n","                steps_trained_in_current_epoch = global_step % (\n","                    len(train_dataloader) // args.gradient_accumulation_steps)\n","\n","                logger.info(\n","                    \"   Continuing training from checkpoint, will skip to saved global_step\"\n","                )\n","                logger.info(\"   Continuing training from epoch %d\",\n","                            epochs_trained)\n","                logger.info(\"   Continuing training from global step %d\",\n","                            global_step)\n","                logger.info(\n","                    \"   Will skip the first %d steps in the current epoch\",\n","                    steps_trained_in_current_epoch)\n","            except ValueError:\n","                logger.info(\"   Starting fine-tuning.\")\n","\n","        if args.evaluate_during_training:\n","            training_progress_scores = self._create_training_progress_scores(\n","                **kwargs)\n","\n","        # if args.wandb_project:\n","        #     wandb.init(project=args.wandb_project, config={**asdict(args)}, **args.wandb_kwargs)\n","        #     wandb.watch(self.model)\n","\n","        if args.fp16:\n","            from torch.cuda import amp\n","\n","            scaler = amp.GradScaler()\n","\n","        self.model.train()\n","        for current_epoch in train_iterator:\n","            if epochs_trained > 0:\n","                epochs_trained -= 1\n","                continue\n","            train_iterator.set_description(\n","                f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\")\n","            batch_iterator = tqdm(\n","                train_dataloader,\n","                desc=f\"Running Epoch {epoch_number} of {args.num_train_epochs}\",\n","                disable=args.silent,\n","                mininterval=0,\n","            )\n","            batch_iterator2 = tqdm(\n","                train_dataloader2,\n","                disable=args.silent,\n","                mininterval=0,\n","            )\n","            print(\"2 done\")\n","            count = 0\n","            for step, batch1 in enumerate(zip(batch_iterator,\n","                                              batch_iterator2)):\n","                print(\"here*********\", steps_trained_in_current_epoch)\n","                if steps_trained_in_current_epoch > 0:\n","                    print(\"here\", steps_trained_in_current_epoch)\n","                    steps_trained_in_current_epoch -= 1\n","                    continue\n","                # batch = tuple(t.to(device) for t in batch)\n","                print(\"range(len(batch1)\", len(batch1), args.train_batch_size)\n","                # print(\"BATCH 1\",batch1)\n","                # batch_p = [self.model.predict(np.array(X_te[i:i+args.train_batch_size]))[i] for i in range(len(batch1))]\n","                # batch_p = [self.model(np.array(X_te[i:i+args.train_batch_size]))[i] for i in range(len(batch1))]\n","                # batch = [np.argmax(batch_p[i], axis=-1) for i in range(len(batch_p))]\n","                batch = batch1\n","                print(\"batch: \", len(batch[0]))\n","                inputs = self._get_inputs_dict(batch[0])\n","                # print(\"batch: \",batch[1])\n","                print(\"inputs\")\n","                print(len(inputs['input_ids']))\n","                print(inputs.keys())\n","                count = count + 1\n","                print(\n","                    \"-----------------------------------------------------------\"\n","                )\n","                if args.fp16:\n","                    with amp.autocast():\n","                        outputs = model(**inputs)\n","                        # model outputs are always tuple in pytorch-transformers (see doc)\n","                        loss = outputs[0]\n","                else:\n","                    outputs = self.model(**inputs)\n","                    # model outputs are always tuple in pytorch-transformers (see doc)\n","                    print(\"outputs\", outputs[2].shape)\n","                    loss = outputs[0]\n","                    print(\"training\", loss)\n","\n","                outputs1 = self.model.generate(\n","                    input_ids=inputs['input_ids'],\n","                    num_beams=self.args.num_beams,\n","                    max_length=self.args.max_length,\n","                    length_penalty=self.args.length_penalty,\n","                    early_stopping=self.args.early_stopping,\n","                    repetition_penalty=self.args.repetition_penalty,\n","                    do_sample=self.args.do_sample,\n","                    top_k=self.args.top_k,\n","                    top_p=self.args.top_p,\n","                    num_return_sequences=self.args.num_return_sequences,\n","                )\n","                outputs_1 = [\n","                    self.decoder_tokenizer.decode(\n","                        output_id,\n","                        skip_special_tokens=True,\n","                        clean_up_tokenization_spaces=True)\n","                    for output_id in outputs1\n","                ]\n","                \n","                ## IMP NOTE: Introducting our loss.\n","                print(type(outputs_1))\n","                print(outputs_1)\n","                a, b, c = tokenize_bert(outputs_1, tokenizer_new)\n","                print(\"a: \", len(a))\n","                print(\"b: \", len(b))\n","                out = model_lr.predict([a, b])\n","                pred_r = [rd[0] for rd in out]\n","                true = batch[1].tolist()\n","                print(\"out: \", pred_r)\n","                print(\"bathc: \", batch[1].tolist())\n","                los1 = mean_squared_error(true, pred_r)\n","                print(\"2nd loss: \", los1)\n","                if args.n_gpu > 1:\n","                    loss = loss.mean(\n","                    )  # mean() to average on multi-gpu parallel training\n","\n","                current_loss = loss.item()\n","                lo = torch.tensor(los1) + loss\n","                print(\"lo:\", lo)\n","                t = los1 + loss\n","                print(\"old loss: \", loss)\n","                loss = loss + los1\n","                print(\"new loss: \", loss)\n","                print(\"t: \", t)\n","                if show_running_loss:\n","                    batch_iterator.set_description(\n","                        f\"Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f}\"\n","                    )\n","\n","                if args.gradient_accumulation_steps > 1:\n","                    loss = loss / args.gradient_accumulation_steps\n","\n","                if args.fp16:\n","                    scaler.scale(loss).backward()\n","                else:\n","                    loss.backward()\n","\n","                tr_loss += loss.item()\n","                if (step + 1) % args.gradient_accumulation_steps == 0:\n","                    if args.fp16:\n","                        scaler.unscale_(optimizer)\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n","                                                   args.max_grad_norm)\n","\n","                    if args.fp16:\n","                        scaler.step(optimizer)\n","                        scaler.update()\n","                    else:\n","                        optimizer.step()\n","                    scheduler.step()  # Update learning rate schedule\n","                    self.model.zero_grad()\n","                    global_step += 1\n","\n","                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                        # Log metrics\n","                        # tb_writer.add_scalar(\"lr\",\n","                        #                      scheduler.get_lr()[0],\n","                        # #                      global_step)\n","                        # tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) /\n","                        #                      args.logging_steps, global_step)\n","                        logging_loss = tr_loss\n","                        # if args.wandb_project:\n","                        #     wandb.log(\n","                        #         {\n","                        #             \"Training loss\": current_loss,\n","                        #             \"lr\": scheduler.get_lr()[0],\n","                        #             \"global_step\": global_step,\n","                        #         }\n","                        #     )\n","\n","                    if args.save_steps > 0: \n","                    # and global_step % args.save_steps == 0:\n","                        # Save model checkpoint\n","                        output_dir_current = os.path.join(\n","                            output_dir, \"checkpoint-{}\".format(global_step))\n","\n","                        self._save_model(output_dir_current,\n","                                         optimizer,\n","                                         scheduler,\n","                                         model=model)  # SAVE COMMENTED\n","\n","                    if args.evaluate_during_training and (\n","                            args.evaluate_during_training_steps > 0\n","                            and global_step %\n","                            args.evaluate_during_training_steps == 0):\n","                        # Only evaluate when single GPU otherwise metrics may not average well\n","                        results = self.eval_model(\n","                            eval_data,\n","                            verbose=verbose\n","                            and args.evaluate_during_training_verbose,\n","                            silent=args.evaluate_during_training_silent,\n","                            **kwargs,\n","                        )\n","                        for key, value in results.items():\n","                          pass\n","                            # tb_writer.add_scalar(\"eval_{}\".format(key), value,\n","                            #                      global_step)\n","\n","                        output_dir_current = os.path.join(\n","                            output_dir, \"checkpoint-{}\".format(global_step))\n","\n","                        if args.save_eval_checkpoints:\n","                            self._save_model(output_dir_current,\n","                                             optimizer,\n","                                             scheduler,\n","                                             model=model,\n","                                             results=results)\n","\n","                        training_progress_scores[\"global_step\"].append(\n","                            global_step)\n","                        training_progress_scores[\"train_loss\"].append(\n","                            current_loss)\n","                        for key in results:\n","                            training_progress_scores[key].append(results[key])\n","                        report = pd.DataFrame(training_progress_scores)\n","                        report.to_csv(\n","                            os.path.join(args.output_dir,\n","                                         \"training_progress_scores.csv\"),\n","                            index=False,\n","                        )\n","\n","                        # if args.wandb_project:\n","                        #     wandb.log(self._get_last_metrics(training_progress_scores))\n","\n","                        if not best_eval_metric:\n","                            best_eval_metric = results[\n","                                args.early_stopping_metric]\n","                            if args.save_best_model:\n","                                self._save_model(args.best_model_dir,\n","                                                 optimizer,\n","                                                 scheduler,\n","                                                 model=model,\n","                                                 results=results)\n","                        if best_eval_metric and args.early_stopping_metric_minimize:\n","                            if results[\n","                                    args.\n","                                    early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n","                                best_eval_metric = results[\n","                                    args.early_stopping_metric]\n","                                if args.save_best_model:\n","                                    self._save_model(args.best_model_dir,\n","                                                     optimizer,\n","                                                     scheduler,\n","                                                     model=model,\n","                                                     results=results)\n","                                early_stopping_counter = 0\n","                            else:\n","                                if args.use_early_stopping:\n","                                    if early_stopping_counter < args.early_stopping_patience:\n","                                        early_stopping_counter += 1\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" No improvement in {args.early_stopping_metric}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Current step: {early_stopping_counter}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                            )\n","                                    else:\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                            )\n","                                            logger.info(\n","                                                \" Training terminated.\")\n","                                            train_iterator.close()\n","                                        return global_step, tr_loss / global_step\n","                        else:\n","                            if results[\n","                                    args.\n","                                    early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n","                                best_eval_metric = results[\n","                                    args.early_stopping_metric]\n","                                if args.save_best_model:\n","                                    self._save_model(args.best_model_dir,\n","                                                     optimizer,\n","                                                     scheduler,\n","                                                     model=model,\n","                                                     results=results)\n","                                early_stopping_counter = 0\n","                            else:\n","                                if args.use_early_stopping:\n","                                    if early_stopping_counter < args.early_stopping_patience:\n","                                        early_stopping_counter += 1\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" No improvement in {args.early_stopping_metric}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Current step: {early_stopping_counter}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                            )\n","                                    else:\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                            )\n","                                            logger.info(\n","                                                \" Training terminated.\")\n","                                            train_iterator.close()\n","                                        return global_step, tr_loss / global_step\n","\n","            print(count)\n","            epoch_number += 1\n","            output_dir_current = os.path.join(\n","                output_dir,\n","                \"checkpoint-{}-epoch-{}\".format(global_step, epoch_number))\n","\n","            if args.save_model_every_epoch or args.evaluate_during_training:\n","                os.makedirs(output_dir_current, exist_ok=True)\n","\n","            if args.save_model_every_epoch:\n","                # self._save_model(output_dir_current, optimizer, scheduler, model=model) # SAVE\n","                pass\n","            if args.evaluate_during_training:\n","                results = self.eval_model(\n","                    eval_data,\n","                    verbose=verbose and args.evaluate_during_training_verbose,\n","                    silent=args.evaluate_during_training_silent,\n","                    **kwargs,\n","                )\n","\n","                if args.save_eval_checkpoints:\n","                    self._save_model(output_dir_current,\n","                                     optimizer,\n","                                     scheduler,\n","                                     results=results)\n","\n","                training_progress_scores[\"global_step\"].append(global_step)\n","                training_progress_scores[\"train_loss\"].append(current_loss)\n","                for key in results:\n","                    training_progress_scores[key].append(results[key])\n","                report = pd.DataFrame(training_progress_scores)\n","                report.to_csv(os.path.join(args.output_dir,\n","                                           \"training_progress_scores.csv\"),\n","                              index=False)\n","\n","                # if args.wandb_project:\n","                #     wandb.log(self._get_last_metrics(training_progress_scores))\n","\n","                if not best_eval_metric:\n","                    best_eval_metric = results[args.early_stopping_metric]\n","                    if args.save_best_model:\n","                        pass\n","                        # self._save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n","                if best_eval_metric and args.early_stopping_metric_minimize:\n","                    if results[\n","                            args.\n","                            early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n","                        best_eval_metric = results[args.early_stopping_metric]\n","                        if args.save_best_model:\n","                            # pass\n","                            self._save_model(args.best_model_dir,\n","                                             optimizer,\n","                                             scheduler,\n","                                             model=model,\n","                                             results=results)  # SAVE COMMENTED\n","                        early_stopping_counter = 0\n","                    else:\n","                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n","                            if early_stopping_counter < args.early_stopping_patience:\n","                                early_stopping_counter += 1\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" No improvement in {args.early_stopping_metric}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Current step: {early_stopping_counter}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                    )\n","                            else:\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                    )\n","                                    logger.info(\" Training terminated.\")\n","                                    train_iterator.close()\n","                                return global_step, tr_loss / global_step\n","                else:\n","                    if results[\n","                            args.\n","                            early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n","                        best_eval_metric = results[args.early_stopping_metric]\n","                        if args.save_best_model:\n","                            # pass\n","                            self._save_model(args.best_model_dir,\n","                                             optimizer,\n","                                             scheduler,\n","                                             model=model,\n","                                             results=results)  # SAVE COMMENTED\n","                        early_stopping_counter = 0\n","                    else:\n","                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n","                            if early_stopping_counter < args.early_stopping_patience:\n","                                early_stopping_counter += 1\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" No improvement in {args.early_stopping_metric}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Current step: {early_stopping_counter}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                    )\n","                            else:\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                    )\n","                                    logger.info(\" Training terminated.\")\n","                                    train_iterator.close()\n","                                return global_step, tr_loss / global_step\n","        print(\"complete first kfold\")\n","        return global_step, tr_loss / global_step\n","\n","    def eval_model(self,\n","                   eval_data,\n","                   output_dir=None,\n","                   verbose=True,\n","                   silent=False,\n","                   **kwargs):\n","        \"\"\"\n","        Evaluates the model on eval_data. Saves results to output_dir.\n","        Args:\n","            eval_data: Pandas DataFrame containing the 2 columns - `input_text`, `target_text`.\n","                        - `input_text`: The input text sequence.\n","                        - `target_text`: The target text sequence.\n","            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n","            verbose: If verbose, results will be printed to the console on completion of evaluation.\n","            silent: If silent, tqdm progress bars will be hidden.\n","            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n","                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n","                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n","        Returns:\n","            results: Dictionary containing evaluation results.\n","        \"\"\"  # noqa: ignore flake8\"\n","\n","        if not output_dir:\n","            output_dir = self.args.output_dir\n","        print(\"eval_data**********\",eval_data)\n","        self._move_model_to_device()\n","\n","        eval_dataset = self.load_and_cache_examples(eval_data,\n","                                                    evaluate=True,\n","                                                    verbose=verbose,\n","                                                    silent=silent)\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        result = self.evaluate(eval_dataset,\n","                               output_dir,\n","                               verbose=verbose,\n","                               silent=silent,\n","                               **kwargs)\n","        self.results.update(result)\n","\n","        if self.args.evaluate_generated_text:\n","            to_predict = eval_data[\"input_text\"].tolist()\n","            preds = self.predict(to_predict)\n","\n","            result = self.compute_metrics(eval_data[\"target_text\"].tolist(),\n","                                          preds, **kwargs)\n","            self.results.update(result)\n","\n","        if verbose:\n","            logger.info(self.results)\n","\n","        return self.results\n","\n","    def evaluate(self,\n","                 eval_dataset,\n","                 output_dir,\n","                 verbose=True,\n","                 silent=False,\n","                 **kwargs):\n","        \"\"\"\n","        Evaluates the model on eval_dataset.\n","        Utility function to be used by the eval_model() method. Not intended to be used directly.\n","        \"\"\"\n","\n","        model = self.model\n","        args = self.args\n","        eval_output_dir = output_dir\n","\n","        results = {}\n","\n","        eval_sampler = SequentialSampler(eval_dataset)\n","        eval_dataloader = DataLoader(eval_dataset,\n","                                     sampler=eval_sampler,\n","                                     batch_size=args.eval_batch_size)\n","\n","        if args.n_gpu > 1:\n","            model = torch.nn.DataParallel(model)\n","\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        model.eval()\n","\n","        for batch in tqdm(eval_dataloader,\n","                          disable=args.silent or silent,\n","                          desc=\"Running Evaluation\"):\n","            # batch = tuple(t.to(device) for t in batch)\n","\n","            inputs = self._get_inputs_dict(batch)\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","                loss = outputs[0]\n","                print(\"Evaluation loss\", loss)\n","                eval_loss += loss.mean().item()\n","            nb_eval_steps += 1\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","\n","        results[\"eval_loss\"] = eval_loss\n","\n","        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            for key in sorted(results.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","        return results\n","\n","    def predict(self, to_predict):\n","        \"\"\"\n","        Performs predictions on a list of text.\n","        Args:\n","            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.\n","        Returns:\n","            preds: A python list of the generated sequences.\n","        \"\"\"  # noqa: ignore flake8\"\n","\n","        self._move_model_to_device()\n","\n","        all_outputs = []\n","        # Batching\n","        for batch in [\n","                to_predict[i:i + self.args.eval_batch_size]\n","                for i in range(0, len(to_predict), self.args.eval_batch_size)\n","        ]:\n","            if self.args.model_type == \"marian\":\n","                input_ids = self.encoder_tokenizer.prepare_translation_batch(\n","                    batch,\n","                    max_length=self.args.max_seq_length,\n","                    pad_to_max_length=True,\n","                    return_tensors=\"pt\",\n","                )[\"input_ids\"]\n","            else:\n","                input_ids = self.encoder_tokenizer.batch_encode_plus(\n","                    batch,\n","                    max_length=self.args.max_seq_length,\n","                    pad_to_max_length=True,\n","                    return_tensors=\"pt\",\n","                )[\"input_ids\"]\n","            # del input_ids[\"special_tokens_mask\"]\n","            input_ids = input_ids.to(self.device)\n","\n","            if self.args.model_type in [\"bart\", \"marian\"]:\n","                outputs = self.model.generate(\n","                    input_ids=input_ids,\n","                    num_beams=self.args.num_beams,\n","                    max_length=self.args.max_length,\n","                    length_penalty=self.args.length_penalty,\n","                    early_stopping=self.args.early_stopping,\n","                    repetition_penalty=self.args.repetition_penalty,\n","                    do_sample=self.args.do_sample,\n","                    top_k=self.args.top_k,\n","                    top_p=self.args.top_p,\n","                    num_return_sequences=self.args.num_return_sequences,\n","                )\n","            else:\n","                outputs = self.model.generate(\n","                    input_ids=input_ids,\n","                    decoder_start_token_id=self.model.config.decoder.\n","                    pad_token_id,\n","                    num_beams=self.args.num_beams,\n","                    max_length=self.args.max_length,\n","                    length_penalty=self.args.length_penalty,\n","                    early_stopping=self.args.early_stopping,\n","                    repetition_penalty=self.args.repetition_penalty,\n","                    do_sample=self.args.do_sample,\n","                    top_k=self.args.top_k,\n","                    top_p=self.args.top_p,\n","                    num_return_sequences=self.args.num_return_sequences,\n","                )\n","\n","            all_outputs.extend(outputs.cpu().numpy())\n","\n","        if self.args.use_multiprocessed_decoding:\n","            self.model.to(\"cpu\")\n","            with Pool(self.args.process_count) as p:\n","                outputs = list(\n","                    tqdm(\n","                        p.imap(self._decode,\n","                               all_outputs,\n","                               chunksize=self.args.multiprocessing_chunksize),\n","                        total=len(all_outputs),\n","                        desc=\"Decoding outputs\",\n","                        disable=self.args.silent,\n","                    ))\n","            self._move_model_to_device()\n","        else:\n","            outputs = [\n","                self.decoder_tokenizer.decode(\n","                    output_id,\n","                    skip_special_tokens=True,\n","                    clean_up_tokenization_spaces=True)\n","                for output_id in all_outputs\n","            ]\n","\n","        if self.args.num_return_sequences > 1:\n","            return [\n","                outputs[i:i + self.args.num_return_sequences]\n","                for i in range(0, len(outputs), self.args.num_return_sequences)\n","            ]\n","        else:\n","            return outputs\n","\n","    def _decode(self, output_id):\n","        return self.decoder_tokenizer.decode(output_id,\n","                                             skip_special_tokens=True,\n","                                             clean_up_tokenization_spaces=True)\n","\n","    def compute_metrics(self, labels, preds, **kwargs):\n","        \"\"\"\n","        Computes the evaluation metrics for the model predictions.\n","        Args:\n","            labels: List of target sequences\n","            preds: List of model generated outputs\n","            **kwargs: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n","                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n","                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n","        Returns:\n","            result: Dictionary containing evaluation results.\n","        \"\"\"  # noqa: ignore flake8\"\n","        # assert len(labels) == len(preds)\n","\n","        results = {}\n","        for metric, func in kwargs.items():\n","            results[metric] = func(labels, preds)\n","\n","        return results\n","\n","    def load_and_cache_examples(self,\n","                                data,\n","                                evaluate=False,\n","                                no_cache=False,\n","                                verbose=True,\n","                                silent=False):\n","        \"\"\"\n","        Creates a T5Dataset from data.\n","        Utility function for train() and eval() methods. Not intended to be used directly.\n","        \"\"\"\n","\n","        encoder_tokenizer = self.encoder_tokenizer\n","        decoder_tokenizer = self.decoder_tokenizer\n","        args = self.args\n","        print(\"data*********************\",data)\n","        if not no_cache:\n","            no_cache = args.no_cache\n","\n","        if not no_cache:\n","            os.makedirs(self.args.cache_dir, exist_ok=True)\n","\n","        mode = \"dev\" if evaluate else \"train\"\n","\n","        if args.dataset_class:\n","            CustomDataset = args.dataset_class\n","            return CustomDataset(encoder_tokenizer, decoder_tokenizer, args,\n","                                 data, mode)\n","        else:\n","            if args.model_type in [\"bart\", \"marian\"]:\n","                return SimpleSummarizationDataset(encoder_tokenizer, self.args,\n","                                                  data, mode)\n","            else:\n","                return Seq2SeqDataset(\n","                    encoder_tokenizer,\n","                    decoder_tokenizer,\n","                    self.args,\n","                    data,\n","                    mode,\n","                )\n","\n","    def _create_training_progress_scores(self, **kwargs):\n","        extra_metrics = {key: [] for key in kwargs}\n","        training_progress_scores = {\n","            \"global_step\": [],\n","            \"eval_loss\": [],\n","            \"train_loss\": [],\n","            **extra_metrics,\n","        }\n","\n","        return training_progress_scores\n","\n","    def _get_last_metrics(self, metric_values):\n","        return {metric: values[-1] for metric, values in metric_values.items()}\n","\n","    def _save_model(self,\n","                    output_dir=None,\n","                    optimizer=None,\n","                    scheduler=None,\n","                    model=None,\n","                    results=None):  # SAVE COMMENTED\n","        if not output_dir:\n","            output_dir = self.args.output_dir\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        logger.info(f\"Saving model into {output_dir}\")\n","        print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n","        print(f\"Saving model into {output_dir}\")\n","\n","        if model and not self.args.no_save:\n","            # Take care of distributed/parallel training\n","            model_to_save = model.module if hasattr(model, \"module\") else model\n","            self._save_model_args(output_dir)\n","\n","            if self.args.model_type in [\"bart\", \"marian\"]:\n","                os.makedirs(os.path.join(output_dir), exist_ok=True)\n","                model_to_save.save_pretrained(output_dir)\n","                self.config.save_pretrained(output_dir)\n","                if self.args.model_type == \"bart\":\n","                    self.encoder_tokenizer.save_pretrained(output_dir)\n","            else:\n","                os.makedirs(os.path.join(output_dir, \"encoder\"), exist_ok=True)\n","                os.makedirs(os.path.join(output_dir, \"decoder\"), exist_ok=True)\n","                self.encoder_config.save_pretrained(\n","                    os.path.join(output_dir, \"encoder\"))\n","                self.decoder_config.save_pretrained(\n","                    os.path.join(output_dir, \"decoder\"))\n","\n","                model_to_save = (self.model.encoder.module if hasattr(\n","                    self.model.encoder, \"module\") else self.model.encoder)\n","                model_to_save.save_pretrained(\n","                    os.path.join(output_dir, \"encoder\"))\n","\n","                model_to_save = (self.model.decoder.module if hasattr(\n","                    self.model.decoder, \"module\") else self.model.decoder)\n","\n","                model_to_save.save_pretrained(\n","                    os.path.join(output_dir, \"decoder\"))\n","\n","                self.encoder_tokenizer.save_pretrained(\n","                    os.path.join(output_dir, \"encoder\"))\n","                self.decoder_tokenizer.save_pretrained(\n","                    os.path.join(output_dir, \"decoder\"))\n","\n","            torch.save(self.args, os.path.join(output_dir,\n","                                               \"training_args.bin\"))\n","            if optimizer and scheduler and self.args.save_optimizer_and_scheduler:\n","                torch.save(optimizer.state_dict(),\n","                           os.path.join(output_dir, \"optimizer.pt\"))\n","                torch.save(scheduler.state_dict(),\n","                           os.path.join(output_dir, \"scheduler.pt\"))\n","\n","        if results:\n","            output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n","            with open(output_eval_file, \"w\") as writer:\n","                for key in sorted(results.keys()):\n","                    writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","    def _move_model_to_device(self):\n","        self.model.to(self.device)\n","\n","    def _get_inputs_dict(self, batch):\n","        device = self.device\n","        if self.args.model_type in [\"bart\", \"marian\"]:\n","            pad_token_id = self.encoder_tokenizer.pad_token_id\n","            source_ids, source_mask, y = batch[\"source_ids\"], batch[\n","                \"source_mask\"], batch[\"target_ids\"]\n","            y_ids = y[:, :-1].contiguous()\n","            lm_labels = y[:, 1:].clone()\n","            lm_labels[y[:, 1:] == pad_token_id] = -100\n","\n","            inputs = {\n","                \"input_ids\": source_ids.to(device),\n","                \"attention_mask\": source_mask.to(device),\n","                \"decoder_input_ids\": y_ids.to(device),\n","                \"lm_labels\": lm_labels.to(device),\n","            }\n","        else:\n","            lm_labels = batch[1]\n","            lm_labels_masked = lm_labels.clone()\n","            lm_labels_masked[lm_labels_masked ==\n","                             self.decoder_tokenizer.pad_token_id] = -100\n","\n","            inputs = {\n","                \"input_ids\": batch[0].to(device),\n","                \"decoder_input_ids\": lm_labels.to(device),\n","                \"labels\": lm_labels_masked.to(device),\n","            }\n","\n","        return inputs\n","\n","    def _save_model_args(self, output_dir):  # SAVE COMMENTED\n","        os.makedirs(output_dir, exist_ok=True)\n","        self.args.save(output_dir)\n","\n","    def _load_model_args(self, input_dir):\n","        args = Seq2SeqArgs()\n","        args.load(input_dir)\n","        return args\n","\n","    def get_named_parameters(self):\n","        return [n for n, p in self.model.named_parameters()]"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-Zf-6y2vVlev","executionInfo":{"status":"ok","timestamp":1659513746275,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}}},"outputs":[],"source":["## Load and train model"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9417d7b8f03743e4a480287f06369651","ae8af51ffb884cb4bde742f4a988aae8","c6df22165c0a46b180c77742711a64b1","48d35725a6a5403481c39705300594d1","0827e4e1cee94d0dac4603395f2d5bd6","d2cfd9368ea54ff7a42a2752183a35e1","2632fb37f52046a19083127571d5603b","7a5b496ffc374fa982aaba9aad75022c","f29db5cdfcf3454495bfd393391ba7d2","633619c690354700b8bd4fb9faac2806","3309b9e844d14b558bfb96f6968b8512","3d228c9dff5d449fb1821ab4dea4e201","f41e32e0e0d54caf8d653e8e2e8faaaa","70dd4a4f341447ebb38a7d24fedd8c94","d8975206497d4d13afbc4bdab3a9b54a","1de02ed7283c4cec8cff12678840fec7","55f86c2f24d34b05a61b8b864657a9d1","7c3b5bce5bbf454ebfb39e22413f4c86","9b5844de02e74a6fa543503c07ab716f","162dd90a258042be90c2ca3e29f2fd5d","0614e0c90d85406e96e56d1c7feea0f5","797b9184d4fb4153809d743641d3b1a6","c80690ffe82c4674a566940ca6b0a3a0","da5c692d4a4540c1b069d223df68f98f","d123bdb38e35403d9fa6f0c50d7b7e0b","b57f8e5592454d39b44b2d58657b0c16","33f830f6fb3c49fa8daebc70ef37ef94","f39e6964d6ac4ddfadfb711ea21df479","abb589a1ae9445a7b89a0fad64bb6527","5b5e79df96a94f3fb8962411909283d5","158ed644355b4ceb9c87146389c8387b","73a6c5c21ab54580a1f52143ca31c71d","878f0e7f852343ef9ad9849dfc65b5e6","dfbc0da045774486bcda54394a70e995","a6ed08fb983d46fab50354b42abcf16f","3a73a9f6ff454ef0a7fbfacb05c95270","b18ce04110154112813a81f396e2689e","b737603db9e3402d88622d484c72aaab","cff82bd161924af5834c748e0785524e","ca2cb4344d6446ccafa296a8be9bc3f6","147cfee6d17f4c1a8f0dabfa017113b5","316de1258e3345cdaa12d3906d6612c9","6ac9ee3ff6ee4b10af4d997212921a75","fb695845c3fc46488dcb00caa6ef8967","818eb0e13d454561a23aa78d8b5b5451","e1e9ebde88a644dcaa9bce0b5be500d6","773721579ece404d9c1e3d164671df7b","438263d16392499297de42fd0ee1d8e3","44a6174670a84a6ca00ef46d57b2ef9a","53a65e0e52ac47399bee1111db434904","da9ee0ca0b544e5f88188b7604d923d0","3096dea577834d34b561a498639c6419","a21b9ba64a4f4d96831293ab638a7cbb","3e0ee9cb103044b28afe40d2060a47e7","0b6226a944a34652b37a35fa7875ab92","3ca989db8a5748cd97f839b666491efb","ee9fc55017454540a04753c6c4fd7e87","8663c932e29c4490ad11f85646670063","11b5fc881d0942f897330ac2423413d6","93d31a36a4b0475ca7eef2c2534acce5","5bc18289e0d94553aab0e06a38373a56","cc6e0930ea63443c8c03bb67c2feef19","de69548f8a8448568501b84083934d2e","0beab515c2a84e5982a4ae05fde4c8da","9402b4302bf44d57b5bffd10af7e0c36","c16b6777e9fb4e489f5289f35d5f42d8","fa7df2986ac24759a00aff7cee8f2a3d","1e5d5497f1ae47e182eb7cca0a321f35","d9bcfb7c1bd44c9f933a5043bf25c8da","23e1b29c3e6c40dbaf257f3df4653e44","ebdb445655644655a591c3db5dc98dbf","f38f0659c25c426ab66ba39f349ef9ef","a60109428d6f466a94fa40281560d2ee","3d78cb512b2d424ba5ed4395e55ebe64","9ab206d8719845e1951ebcd22fb984a0","e1bdbce6e1f84e8a8608cf1881b183a7","be7658d30fe84f0193200c406961ada3","f7b605bfbadb44ddbfdd14b19851a53b","b045a17e2a65472ea0f48ce2bd3896bd","02f3a43d71b64deb85ca8298300f70c9","e37b409690ec4815befe5b32e72878b9","1af1896b63d2438fb3600655bbc52210","e6ad066f904d4bd2bb5f393a0a490ab3","135b7e03bc7142d1b23b12097eb58acb","212af06465004b549469e13a9192c0c6","3d5e4c5dae284841b365b7edebef073d","f5d84e968e8c45cd802d16eb7429b509","d7969e1fe9824b90b20783c258bae661"]},"id":"Ngmwqw4VVlew","executionInfo":{"status":"ok","timestamp":1659513930358,"user_tz":-330,"elapsed":183352,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}},"outputId":"1a7fffdf-346f-4eb6-caab-e4c93835531c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9417d7b8f03743e4a480287f06369651"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["in train_model**** eval_data    index  Unnamed: 0                                         input_text  \\\n","0      0           0  What I do know is that I live on a small Islan...   \n","1      1           1  #BuildThatWall #BuildTheWall #BuildTheDamnWall...   \n","2      2           2  Couple Befriend Negroid, Negroid Tries To Burn...   \n","3      3           3  i've felt anger when i've read it on forum, bu...   \n","4      4           4  Well, the warm weather is coming in and guess ...   \n","5      5           5  The jew media likes to prop this whites becomi...   \n","6      6           6  DuetschGirlTX, No I live in a tiny ass town wh...   \n","7      7           7  I lost several friends over openly supporting ...   \n","8      8           8  The protesters banged in droves on the glass f...   \n","9      9           9  Its a bitch who better get her some business f...   \n","\n","                                         target_text      prefix  \n","0  What I do know is that I live on a small Islan...  paraphrase  \n","1  Ask them to leave the nation. Ask them to leav...  paraphrase  \n","2  Couple Befriend person, person Tries To Burn D...  paraphrase  \n","3  i've felt anger when i've read it on forum, bu...  paraphrase  \n","4  Well, the warm weather is coming in and guess ...  paraphrase  \n","5  The Jew media likes to say that the people are...  paraphrase  \n","6  DuetschGirlTX, No I live in a small town where...  paraphrase  \n","7  I lost several friends over openly supporting ...  paraphrase  \n","8  The protesters banged in droves on the glass f...  paraphrase  \n","9  Its a girl who better get her some business fo...  paraphrase  \n","train\n","10\n","   index  Unnamed: 0                                         input_text  \\\n","0     10          10  Aren't u one of the low life scum who defends ...   \n","1     11          11  If #WeThePeople want a #WALL - Give us a WALL!...   \n","2     12          12  Mr @user #Trump, Respectfully, sir, if you wan...   \n","3     13          13  whats up I thought I might put my pic up here ...   \n","4     14          14  BTW you are not wrong about Mildura, I went th...   \n","5     15          15  Is the Judge On Crack? he Said said Govt needs...   \n","6     16          16  This family that wants to complain about gays ...   \n","7     17          17  Sort of a back handed compliment that the anti...   \n","8     18          18  i am just down the road to you.welcome to the ...   \n","9     19          19  @user I speak for myself & EVERYONE I KNOW her...   \n","\n","                                         target_text      prefix  \n","0  Aren't u one of the low life scum who defends ...  paraphrase  \n","1  If want a Ask them to leave the nation. - Give...  paraphrase  \n","2  Mr  Respectfully, sir, if you want to run the ...  paraphrase  \n","3  whats up I thought I might put my pic up here ...  paraphrase  \n","4  BTW you are not wrong about that place, I went...  paraphrase  \n","5  Is the Judge drugged? he Said said Govt needs ...  paraphrase  \n","6  This family that wants to complain about peopl...  paraphrase  \n","7  Sort of a back handed compliment that the peop...  paraphrase  \n","8  i am just down the road to you.welcome to the ...  paraphrase  \n","9  I speak for myself &amp; EVERYONE I KNOW here ...  paraphrase  \n","labels:  [7.0862575 7.545284  7.664287  5.442789  6.373203  7.15841   5.524738\n"," 5.605469  6.4337344 6.8168435]\n","type:  <class 'numpy.ndarray'>\n","Beginiing 1st kfold\n","data*********************    index  Unnamed: 0                                         input_text  \\\n","0     10          10  Aren't u one of the low life scum who defends ...   \n","1     11          11  If #WeThePeople want a #WALL - Give us a WALL!...   \n","2     12          12  Mr @user #Trump, Respectfully, sir, if you wan...   \n","3     13          13  whats up I thought I might put my pic up here ...   \n","4     14          14  BTW you are not wrong about Mildura, I went th...   \n","5     15          15  Is the Judge On Crack? he Said said Govt needs...   \n","6     16          16  This family that wants to complain about gays ...   \n","7     17          17  Sort of a back handed compliment that the anti...   \n","8     18          18  i am just down the road to you.welcome to the ...   \n","9     19          19  @user I speak for myself & EVERYONE I KNOW her...   \n","\n","                                         target_text      prefix  \n","0  Aren't u one of the low life scum who defends ...  paraphrase  \n","1  If want a Ask them to leave the nation. - Give...  paraphrase  \n","2  Mr  Respectfully, sir, if you want to run the ...  paraphrase  \n","3  whats up I thought I might put my pic up here ...  paraphrase  \n","4  BTW you are not wrong about that place, I went...  paraphrase  \n","5  Is the Judge drugged? he Said said Govt needs ...  paraphrase  \n","6  This family that wants to complain about peopl...  paraphrase  \n","7  Sort of a back handed compliment that the peop...  paraphrase  \n","8  i am just down the road to you.welcome to the ...  paraphrase  \n","9  I speak for myself &amp; EVERYONE I KNOW here ...  paraphrase  \n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d228c9dff5d449fb1821ab4dea4e201"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","ln 10\n","1 done\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c80690ffe82c4674a566940ca6b0a3a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Running Epoch 0 of 1:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfbc0da045774486bcda54394a70e995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818eb0e13d454561a23aa78d8b5b5451"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["2 done\n","here********* 0\n","range(len(batch1) 2 8\n","batch:  3\n","inputs\n","8\n","dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'lm_labels'])\n","-----------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/modeling_bart.py:1075: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["outputs torch.Size([8, 128, 768])\n","training tensor(2.7970, grad_fn=<NllLossBackward>)\n","<class 'list'>\n","[\"Aren't u one of the low life scum who defends people bringing fentanyl into this country from Mexico? Sorry, but I won't apologize for being MORE concerned abt the 70,000 ppl dying from drug overdoses than people sneaking into the country illegally #BuildTheWall #BuildThatWall \", \"If #WeThePeople want a #WALL - Give us a WALL!! #Congress works for US, not themselves and the Special Interests who pour BILLIONS of dollars into their Coffers. It's not up to them, it's up to US! #BuildTheWall #BuildThatWall #MAGA #KAG \", 'Mr @user #Trump, Respectfully, sir, if you want to run the #2018Midterms on the  #Pentagon Will #BuildThatWall, a then you must show your #MAGA BASE that you are actually going to do it. No empty promises. No silly games. No BS nonsense. Start the process now! #BuildTheWall. @url ', \"whats up I thought I might put my pic up here I'm looking for some friends theres too many negroes around here some times I feel like there is'int any white guy left in the world. so message me and let me know there is stll some white brothers out their.\", \"BTW you are not wrong about Mildura, I went through there in mid 2000 and there wasn't a non-White in sight but late last year when I went there all I saw were islanders, indians, asians, and whatever other crap little johnnie could get dumped into the country. \", 'Is the Judge On Crack? he said said Govt needs to prove it considered the benefits of having illegal Aliens here and working! Illegal Immigration cost $ 135 B a year while Ilals pay $! 8 B in taxes! Illegals are No Benefit to the U.S #Trump #MAGA @url', \"This family that wants to complain about gays should come to San Francisco and raise some hell at these gay pride events... a much better target... and these people in San Francisco openly blasphemied Christ with their parody of'The Last Supper ` ` as their poster for their last event.\", 'Sort of a back handed compliment that the anti-Whites are so keen to come on here and tell us that all is lost - in their heart of hearts even they must know that what they have constructed is only temporary and all will be ripped down some day.']\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca989db8a5748cd97f839b666491efb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["a:  8\n","b:  8\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["out:  [7.0862575, 7.545284, 7.664287, 5.442789, 6.4447665, 7.1556334, 5.524736, 5.6054688]\n","bathc:  [7.086257457733154, 7.545283794403076, 7.664287090301514, 5.442789077758789, 6.373202800750732, 7.15841007232666, 5.52473783493042, 5.605469226837158]\n","2nd loss:  0.0006411344698449284\n","lo: tensor(2.7977, grad_fn=<AddBackward0>)\n","old loss:  tensor(2.7970, grad_fn=<NllLossBackward>)\n","new loss:  tensor(2.7977, grad_fn=<AddBackward0>)\n","t:  tensor(2.7977, grad_fn=<AddBackward0>)\n","here********* 0\n","range(len(batch1) 2 8\n","batch:  3\n","inputs\n","2\n","dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'lm_labels'])\n","-----------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/modeling_bart.py:1075: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["outputs torch.Size([2, 128, 768])\n","training tensor(2.2461, grad_fn=<NllLossBackward>)\n","<class 'list'>\n","['i am just down the road to you.welcome to the board.it breaks my heart every time i go to Galway city and see the amount of blacks there.the other week i was walking down shop street and there was a moslem standing in the street selling copies of the koran', \"@user I speak for myself & EVERYONE I KNOW here on Big Island - vote YES for funding to #BUILDtheWALL. Never in my years here have I heard the concerns from every level. The young in Hilo losing jobs to migrants. Several auto accidents, with'u\"]\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa7df2986ac24759a00aff7cee8f2a3d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["a:  2\n","b:  2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["out:  [6.4337344, 6.8168435]\n","bathc:  [6.43373441696167, 6.816843509674072]\n","2nd loss:  0.0\n","lo: tensor(2.2461, grad_fn=<AddBackward0>)\n","old loss:  tensor(2.2461, grad_fn=<NllLossBackward>)\n","new loss:  tensor(2.2461, grad_fn=<AddBackward0>)\n","t:  tensor(2.2461, grad_fn=<AddBackward0>)\n","2\n","eval_data**********    index  Unnamed: 0                                         input_text  \\\n","0      0           0  What I do know is that I live on a small Islan...   \n","1      1           1  #BuildThatWall #BuildTheWall #BuildTheDamnWall...   \n","2      2           2  Couple Befriend Negroid, Negroid Tries To Burn...   \n","3      3           3  i've felt anger when i've read it on forum, bu...   \n","4      4           4  Well, the warm weather is coming in and guess ...   \n","5      5           5  The jew media likes to prop this whites becomi...   \n","6      6           6  DuetschGirlTX, No I live in a tiny ass town wh...   \n","7      7           7  I lost several friends over openly supporting ...   \n","8      8           8  The protesters banged in droves on the glass f...   \n","9      9           9  Its a bitch who better get her some business f...   \n","\n","                                         target_text      prefix  \n","0  What I do know is that I live on a small Islan...  paraphrase  \n","1  Ask them to leave the nation. Ask them to leav...  paraphrase  \n","2  Couple Befriend person, person Tries To Burn D...  paraphrase  \n","3  i've felt anger when i've read it on forum, bu...  paraphrase  \n","4  Well, the warm weather is coming in and guess ...  paraphrase  \n","5  The Jew media likes to say that the people are...  paraphrase  \n","6  DuetschGirlTX, No I live in a small town where...  paraphrase  \n","7  I lost several friends over openly supporting ...  paraphrase  \n","8  The protesters banged in droves on the glass f...  paraphrase  \n","9  Its a girl who better get her some business fo...  paraphrase  \n","data*********************    index  Unnamed: 0                                         input_text  \\\n","0      0           0  What I do know is that I live on a small Islan...   \n","1      1           1  #BuildThatWall #BuildTheWall #BuildTheDamnWall...   \n","2      2           2  Couple Befriend Negroid, Negroid Tries To Burn...   \n","3      3           3  i've felt anger when i've read it on forum, bu...   \n","4      4           4  Well, the warm weather is coming in and guess ...   \n","5      5           5  The jew media likes to prop this whites becomi...   \n","6      6           6  DuetschGirlTX, No I live in a tiny ass town wh...   \n","7      7           7  I lost several friends over openly supporting ...   \n","8      8           8  The protesters banged in droves on the glass f...   \n","9      9           9  Its a bitch who better get her some business f...   \n","\n","                                         target_text      prefix  \n","0  What I do know is that I live on a small Islan...  paraphrase  \n","1  Ask them to leave the nation. Ask them to leav...  paraphrase  \n","2  Couple Befriend person, person Tries To Burn D...  paraphrase  \n","3  i've felt anger when i've read it on forum, bu...  paraphrase  \n","4  Well, the warm weather is coming in and guess ...  paraphrase  \n","5  The Jew media likes to say that the people are...  paraphrase  \n","6  DuetschGirlTX, No I live in a small town where...  paraphrase  \n","7  I lost several friends over openly supporting ...  paraphrase  \n","8  The protesters banged in droves on the glass f...  paraphrase  \n","9  Its a girl who better get her some business fo...  paraphrase  \n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b605bfbadb44ddbfdd14b19851a53b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/modeling_bart.py:1075: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Evaluation loss tensor(2.4476)\n","Evaluation loss tensor(0.7612)\n","Evaluation loss tensor(1.6414)\n","Evaluation loss tensor(1.9167)\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Evaluation loss tensor(1.8258)\n","complete first kfold\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["logger = logging.getLogger(__name__)\n","\n","train_df = pd.read_csv(BASE_FOLDER + BART_INPUT)\n","\n","test_df = train_df[:10]\n","test_df = test_df.reset_index()\n","train_df = train_df[10:20]\n","train_df = train_df.reset_index()\n","\n","train_input_ids, train_input_masks, train_input_segment = tokenize_bert(\n","    train_df[\"input_text\"], tokenizer_intensity)\n","train_int_labels = intensity_model.predict(\n","    x=[train_input_ids, train_input_masks]).flatten()\n","\n","model_args = Seq2SeqArgs()\n","model_args.do_sample = True\n","model_args.eval_batch_size = 2\n","model_args.evaluate_during_training = True\n","model_args.evaluate_during_training_steps = 2500\n","model_args.evaluate_during_training_verbose = True\n","model_args.fp16 = False\n","model_args.learning_rate = 5e-5\n","model_args.max_length = 128\n","model_args.max_seq_length = 128\n","model_args.num_beams = None\n","model_args.num_return_sequences = 1\n","model_args.num_train_epochs = 1\n","model_args.overwrite_output_dir = True\n","model_args.reprocess_input_data = True\n","model_args.save_eval_checkpoints = False\n","model_args.save_steps = -1\n","model_args.top_k = 50\n","model_args.top_p = 0.95\n","model_args.train_batch_size = 8\n","model_args.use_multiprocessing = False\n","\n","model_args.from_tf = True\n","\n","model = Seq2SeqModel(encoder_decoder_type=\"bart\",\n","                     encoder_decoder_name=\"facebook/bart-base\",\n","                     args=model_args,\n","                     use_cuda=False,\n","                     from_tf=True,\n","                     force_download=True)\n","model.train_model(train_data=train_df,\n","                  model_lr=intensity_model,\n","                  tokenizer_new=tokenizer_intensity,\n","                  labels=train_int_labels,\n","                  eval_data = test_df,\n","                  test_data=test_df)\n","output.close()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"5wQHcO6cVlew","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659514006532,"user_tz":-330,"elapsed":76191,"user":{"displayName":"Sarah Masud","userId":"01097683761850841511"}},"outputId":"e6335cbe-879e-413e-cc9a-6a25bd30d5b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["['What I do know is that I live on a small Island and it seems that a lot of people from various countries in the world are willing to desert their homelands and their families just to come here, in my mind that tells me a lot about said people and their home countries, most of all it highlights their weaknesses of how their own countries have let them down and how they feel the need to feed off somebody elses country, how should we English interpret this without sounding rude???, locust???, parasite???, you tell me. ', \"#BuildThatWall #BuildTheWall #buildThatWall TheBuildTheDamnWall # BuildThatWallTheBuildThe DamnWall BuildingThatWall BuildingTheDamn Wall BuildingThat Wall #BuildThat Wall BuildingThe Damn Wall Building That Wall BuildThat Wall TheBuild ThatWall BuildTheWall BUILD THAT EFFEN WALL! I'm sorry my Lord #Jesus but people are just deaf down here\", 'Couple Befriend Negroid, Negroid Tries To Burn Down Couples Home - NNN Reporters Newsroom Forum Moblie Couple Says Neighbor Tried to Set Their Apartment on Fire Reported by : Irika Sargent Last Update : 4/25 9 : 01 pm Moblie Couple Says Neighbor Did Not Want to Set their Apartment On Fire | Local15tv.com, Mobile & Pensacola News, Entertainment, Videos, Business Search and Shopping This is what happens when you make friends with spearchuckers. ', \"i've felt anger when i've read it on forum, but we have to do our part and speaking at loud voice this fact, because, for example, i'm from europe and this fact is not pased on tv news or in the news paper... knows your enemies is the first pass for vicory on them! \", 'Well, the warm weather is coming in and guess what, I now have to painstakingly endure all the offspring of these immigrants, Africans, foeigners ( Im putting them all because there is that many of them ) all playing out the front of the flats in the small garden area, screeching and shouting and their hyena pitched laughing. ', 'The jew media likes to prop this whites becoming the minority non sense up because they want to try and scare whitey, the media plays on the fears that the average retard thinks hispanic or latino is a race and they play word play with titles like white hispanic and white non hispanic it is all a magic show nothing more.. But Whites becoming a minority in Europe the homeland is a much more serious issue that concerns me greatly. ', 'DuetschGirlTX, No I live in a tiny ass town where i am only one of 100 white folks ( maybe less ). near San Antonio. there are so many dirts around and they all stick together. so when it comes to a White Girl in this town i gotta have my own back. Is that where you are? ', 'I lost several friends over openly supporting & voting for @user @user & one of the main reasons I voted for him was his promise to build a Border Wall & so help me if he doesnt Ill sit home in 2020! #BuildThatWall #BuildTheWallNow #Build theWall @user', \"The protesters banged in droves on the glass for the entire time I was in North Korea, Mao's grandson Mao Xinyu appeared at a meeting in Beijing on Friday. There is no review requirement under the Constitution to #BuildThatWall as a national defense issue - #BuildTheWall \", \"Its a bitch who better get her some business forreal! How the fuck u gone feel entitled to somebody! Bitch u got blackballed & u gone stay blackballing! I ain't saving no hoe! U is not apart of my Spurs dreams! The woman u see on my page had on the San Antonio Spurs Warm up Jacket\"]\n"]}],"source":["to_predict = [\n","    str(input_text)\n","    for input_text in test_df[\"input_text\"].tolist()\n","]\n","\n","out=model.predict(to_predict)\n","print(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OtrQmk2Vlex"},"outputs":[],"source":["# model.save_pretrained(BASE_FOLDER + OUTPUT_FOLDER + OUTPUT_FILE)"]},{"cell_type":"code","source":[""],"metadata":{"id":"JdSBoaNbV7qB"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"hate_int_reduce.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bb9dfc3e6a7b48daa91c5fbb46a0fd9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7be617edc034c1285104c5a7deebcbc","IPY_MODEL_33ba4a378cca4b00a412fcfd9460291f","IPY_MODEL_a35568ad47aa42ff9273b98664c15556"],"layout":"IPY_MODEL_a83747dcddc347349cf7cb83c4831528"}},"b7be617edc034c1285104c5a7deebcbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6513065e7e9f4ad4be0837928a5208b0","placeholder":"​","style":"IPY_MODEL_225fbc26c7684e6eb898ac74edd0687d","value":"Downloading: 100%"}},"33ba4a378cca4b00a412fcfd9460291f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e66aa8eaa920463c90369000fbb7b71d","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84abd224d96e4971aacd239110e92606","value":231508}},"a35568ad47aa42ff9273b98664c15556":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8492f202e8f433e9462c523c5720996","placeholder":"​","style":"IPY_MODEL_b164a10f2e5a40b4b0a19a10e4dd4f52","value":" 232k/232k [00:00&lt;00:00, 1.64MB/s]"}},"a83747dcddc347349cf7cb83c4831528":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6513065e7e9f4ad4be0837928a5208b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"225fbc26c7684e6eb898ac74edd0687d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e66aa8eaa920463c90369000fbb7b71d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84abd224d96e4971aacd239110e92606":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8492f202e8f433e9462c523c5720996":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b164a10f2e5a40b4b0a19a10e4dd4f52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fb0b06f0a1d4492afe5fe3133b763f6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfd00e805b504330b5e391d96fab9e98","IPY_MODEL_a127858d3e8e4cb2a270700125749799","IPY_MODEL_714a3a08456c45988c048daf8e27d4c7"],"layout":"IPY_MODEL_1dbfd5c860f54d4ab03e2631223dab21"}},"cfd00e805b504330b5e391d96fab9e98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4079b20f61c54689adfb758796c99e63","placeholder":"​","style":"IPY_MODEL_91db461ea79a4dd7b9965d82504b5844","value":"Downloading: 100%"}},"a127858d3e8e4cb2a270700125749799":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f64a6ad84cc24d03a33cfeadf011c13b","max":363423424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ac8209000b34ee9b44aa9fc975416b7","value":363423424}},"714a3a08456c45988c048daf8e27d4c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab80eeaaa9e544cc8fa3fa83bc7b5346","placeholder":"​","style":"IPY_MODEL_7ce56e0ef85d4c248d3236e7a77b9e10","value":" 363M/363M [00:10&lt;00:00, 21.0MB/s]"}},"1dbfd5c860f54d4ab03e2631223dab21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4079b20f61c54689adfb758796c99e63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91db461ea79a4dd7b9965d82504b5844":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f64a6ad84cc24d03a33cfeadf011c13b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ac8209000b34ee9b44aa9fc975416b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ab80eeaaa9e544cc8fa3fa83bc7b5346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ce56e0ef85d4c248d3236e7a77b9e10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9417d7b8f03743e4a480287f06369651":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae8af51ffb884cb4bde742f4a988aae8","IPY_MODEL_c6df22165c0a46b180c77742711a64b1","IPY_MODEL_48d35725a6a5403481c39705300594d1"],"layout":"IPY_MODEL_0827e4e1cee94d0dac4603395f2d5bd6"}},"ae8af51ffb884cb4bde742f4a988aae8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2cfd9368ea54ff7a42a2752183a35e1","placeholder":"​","style":"IPY_MODEL_2632fb37f52046a19083127571d5603b","value":"100%"}},"c6df22165c0a46b180c77742711a64b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a5b496ffc374fa982aaba9aad75022c","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f29db5cdfcf3454495bfd393391ba7d2","value":10}},"48d35725a6a5403481c39705300594d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_633619c690354700b8bd4fb9faac2806","placeholder":"​","style":"IPY_MODEL_3309b9e844d14b558bfb96f6968b8512","value":" 10/10 [00:00&lt;00:00, 89.55it/s]"}},"0827e4e1cee94d0dac4603395f2d5bd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2cfd9368ea54ff7a42a2752183a35e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2632fb37f52046a19083127571d5603b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a5b496ffc374fa982aaba9aad75022c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f29db5cdfcf3454495bfd393391ba7d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"633619c690354700b8bd4fb9faac2806":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3309b9e844d14b558bfb96f6968b8512":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d228c9dff5d449fb1821ab4dea4e201":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f41e32e0e0d54caf8d653e8e2e8faaaa","IPY_MODEL_70dd4a4f341447ebb38a7d24fedd8c94","IPY_MODEL_d8975206497d4d13afbc4bdab3a9b54a"],"layout":"IPY_MODEL_1de02ed7283c4cec8cff12678840fec7"}},"f41e32e0e0d54caf8d653e8e2e8faaaa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55f86c2f24d34b05a61b8b864657a9d1","placeholder":"​","style":"IPY_MODEL_7c3b5bce5bbf454ebfb39e22413f4c86","value":"100%"}},"70dd4a4f341447ebb38a7d24fedd8c94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b5844de02e74a6fa543503c07ab716f","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_162dd90a258042be90c2ca3e29f2fd5d","value":10}},"d8975206497d4d13afbc4bdab3a9b54a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0614e0c90d85406e96e56d1c7feea0f5","placeholder":"​","style":"IPY_MODEL_797b9184d4fb4153809d743641d3b1a6","value":" 10/10 [00:00&lt;00:00, 121.21it/s]"}},"1de02ed7283c4cec8cff12678840fec7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55f86c2f24d34b05a61b8b864657a9d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c3b5bce5bbf454ebfb39e22413f4c86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b5844de02e74a6fa543503c07ab716f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"162dd90a258042be90c2ca3e29f2fd5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0614e0c90d85406e96e56d1c7feea0f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"797b9184d4fb4153809d743641d3b1a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c80690ffe82c4674a566940ca6b0a3a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da5c692d4a4540c1b069d223df68f98f","IPY_MODEL_d123bdb38e35403d9fa6f0c50d7b7e0b","IPY_MODEL_b57f8e5592454d39b44b2d58657b0c16"],"layout":"IPY_MODEL_33f830f6fb3c49fa8daebc70ef37ef94"}},"da5c692d4a4540c1b069d223df68f98f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f39e6964d6ac4ddfadfb711ea21df479","placeholder":"​","style":"IPY_MODEL_abb589a1ae9445a7b89a0fad64bb6527","value":"Epoch 1 of 1: 100%"}},"d123bdb38e35403d9fa6f0c50d7b7e0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b5e79df96a94f3fb8962411909283d5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_158ed644355b4ceb9c87146389c8387b","value":1}},"b57f8e5592454d39b44b2d58657b0c16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a6c5c21ab54580a1f52143ca31c71d","placeholder":"​","style":"IPY_MODEL_878f0e7f852343ef9ad9849dfc65b5e6","value":" 1/1 [01:34&lt;00:00, 94.57s/it]"}},"33f830f6fb3c49fa8daebc70ef37ef94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f39e6964d6ac4ddfadfb711ea21df479":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abb589a1ae9445a7b89a0fad64bb6527":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b5e79df96a94f3fb8962411909283d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"158ed644355b4ceb9c87146389c8387b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73a6c5c21ab54580a1f52143ca31c71d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"878f0e7f852343ef9ad9849dfc65b5e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfbc0da045774486bcda54394a70e995":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6ed08fb983d46fab50354b42abcf16f","IPY_MODEL_3a73a9f6ff454ef0a7fbfacb05c95270","IPY_MODEL_b18ce04110154112813a81f396e2689e"],"layout":"IPY_MODEL_b737603db9e3402d88622d484c72aaab"}},"a6ed08fb983d46fab50354b42abcf16f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cff82bd161924af5834c748e0785524e","placeholder":"​","style":"IPY_MODEL_ca2cb4344d6446ccafa296a8be9bc3f6","value":"Epochs 0/1. Running Loss:    2.2461: 100%"}},"3a73a9f6ff454ef0a7fbfacb05c95270":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_147cfee6d17f4c1a8f0dabfa017113b5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_316de1258e3345cdaa12d3906d6612c9","value":2}},"b18ce04110154112813a81f396e2689e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ac9ee3ff6ee4b10af4d997212921a75","placeholder":"​","style":"IPY_MODEL_fb695845c3fc46488dcb00caa6ef8967","value":" 2/2 [01:27&lt;00:00, 39.81s/it]"}},"b737603db9e3402d88622d484c72aaab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cff82bd161924af5834c748e0785524e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca2cb4344d6446ccafa296a8be9bc3f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"147cfee6d17f4c1a8f0dabfa017113b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316de1258e3345cdaa12d3906d6612c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ac9ee3ff6ee4b10af4d997212921a75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb695845c3fc46488dcb00caa6ef8967":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"818eb0e13d454561a23aa78d8b5b5451":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1e9ebde88a644dcaa9bce0b5be500d6","IPY_MODEL_773721579ece404d9c1e3d164671df7b","IPY_MODEL_438263d16392499297de42fd0ee1d8e3"],"layout":"IPY_MODEL_44a6174670a84a6ca00ef46d57b2ef9a"}},"e1e9ebde88a644dcaa9bce0b5be500d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53a65e0e52ac47399bee1111db434904","placeholder":"​","style":"IPY_MODEL_da9ee0ca0b544e5f88188b7604d923d0","value":" 50%"}},"773721579ece404d9c1e3d164671df7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3096dea577834d34b561a498639c6419","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a21b9ba64a4f4d96831293ab638a7cbb","value":1}},"438263d16392499297de42fd0ee1d8e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e0ee9cb103044b28afe40d2060a47e7","placeholder":"​","style":"IPY_MODEL_0b6226a944a34652b37a35fa7875ab92","value":" 1/2 [01:27&lt;01:05, 65.67s/it]"}},"44a6174670a84a6ca00ef46d57b2ef9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53a65e0e52ac47399bee1111db434904":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da9ee0ca0b544e5f88188b7604d923d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3096dea577834d34b561a498639c6419":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a21b9ba64a4f4d96831293ab638a7cbb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3e0ee9cb103044b28afe40d2060a47e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b6226a944a34652b37a35fa7875ab92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ca989db8a5748cd97f839b666491efb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee9fc55017454540a04753c6c4fd7e87","IPY_MODEL_8663c932e29c4490ad11f85646670063","IPY_MODEL_11b5fc881d0942f897330ac2423413d6"],"layout":"IPY_MODEL_93d31a36a4b0475ca7eef2c2534acce5"}},"ee9fc55017454540a04753c6c4fd7e87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bc18289e0d94553aab0e06a38373a56","placeholder":"​","style":"IPY_MODEL_cc6e0930ea63443c8c03bb67c2feef19","value":"100%"}},"8663c932e29c4490ad11f85646670063":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de69548f8a8448568501b84083934d2e","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0beab515c2a84e5982a4ae05fde4c8da","value":8}},"11b5fc881d0942f897330ac2423413d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9402b4302bf44d57b5bffd10af7e0c36","placeholder":"​","style":"IPY_MODEL_c16b6777e9fb4e489f5289f35d5f42d8","value":" 8/8 [00:00&lt;00:00, 110.19it/s]"}},"93d31a36a4b0475ca7eef2c2534acce5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bc18289e0d94553aab0e06a38373a56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc6e0930ea63443c8c03bb67c2feef19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de69548f8a8448568501b84083934d2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0beab515c2a84e5982a4ae05fde4c8da":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9402b4302bf44d57b5bffd10af7e0c36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c16b6777e9fb4e489f5289f35d5f42d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa7df2986ac24759a00aff7cee8f2a3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e5d5497f1ae47e182eb7cca0a321f35","IPY_MODEL_d9bcfb7c1bd44c9f933a5043bf25c8da","IPY_MODEL_23e1b29c3e6c40dbaf257f3df4653e44"],"layout":"IPY_MODEL_ebdb445655644655a591c3db5dc98dbf"}},"1e5d5497f1ae47e182eb7cca0a321f35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f38f0659c25c426ab66ba39f349ef9ef","placeholder":"​","style":"IPY_MODEL_a60109428d6f466a94fa40281560d2ee","value":"100%"}},"d9bcfb7c1bd44c9f933a5043bf25c8da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d78cb512b2d424ba5ed4395e55ebe64","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ab206d8719845e1951ebcd22fb984a0","value":2}},"23e1b29c3e6c40dbaf257f3df4653e44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1bdbce6e1f84e8a8608cf1881b183a7","placeholder":"​","style":"IPY_MODEL_be7658d30fe84f0193200c406961ada3","value":" 2/2 [00:00&lt;00:00, 47.80it/s]"}},"ebdb445655644655a591c3db5dc98dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f38f0659c25c426ab66ba39f349ef9ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a60109428d6f466a94fa40281560d2ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d78cb512b2d424ba5ed4395e55ebe64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ab206d8719845e1951ebcd22fb984a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1bdbce6e1f84e8a8608cf1881b183a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be7658d30fe84f0193200c406961ada3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7b605bfbadb44ddbfdd14b19851a53b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b045a17e2a65472ea0f48ce2bd3896bd","IPY_MODEL_02f3a43d71b64deb85ca8298300f70c9","IPY_MODEL_e37b409690ec4815befe5b32e72878b9"],"layout":"IPY_MODEL_1af1896b63d2438fb3600655bbc52210"}},"b045a17e2a65472ea0f48ce2bd3896bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6ad066f904d4bd2bb5f393a0a490ab3","placeholder":"​","style":"IPY_MODEL_135b7e03bc7142d1b23b12097eb58acb","value":"100%"}},"02f3a43d71b64deb85ca8298300f70c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_212af06465004b549469e13a9192c0c6","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d5e4c5dae284841b365b7edebef073d","value":10}},"e37b409690ec4815befe5b32e72878b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5d84e968e8c45cd802d16eb7429b509","placeholder":"​","style":"IPY_MODEL_d7969e1fe9824b90b20783c258bae661","value":" 10/10 [00:00&lt;00:00, 118.60it/s]"}},"1af1896b63d2438fb3600655bbc52210":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6ad066f904d4bd2bb5f393a0a490ab3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"135b7e03bc7142d1b23b12097eb58acb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"212af06465004b549469e13a9192c0c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d5e4c5dae284841b365b7edebef073d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5d84e968e8c45cd802d16eb7429b509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7969e1fe9824b90b20783c258bae661":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}