{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Span Prediction (HSP): Multi-class Classification\n",
    "\n",
    "HSP Module takes a non-normalised sentence and predicts hateful spans within the spans. These spans are initially manualled annotated via the BIO notation. \n",
    "For evaluation we get the classic P/R/F1 but through the lense of POS tagged sequence eval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The folder is setup to from google drive. If used else only the following lines needs commenting\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMP NOTE: the trainable elmo crf module works only in tf 1. Tf-hub does not effectively support training of elmo in tf2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install these inside colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.13.1\n",
    "# !pip install tensorflow-gpu==1.13.1\n",
    "# !pip install tensorflow-hub==0.7.0\n",
    "# !pip install keras==2.2.4\n",
    "# !pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "# !pip install seqeval\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSu2VigSMHwb"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkEbtvMhJ6O7"
   },
   "outputs": [],
   "source": [
    "BASE_FOLDER = \"/content/drive/MyDrive/hate_norm_kdd22/\"\n",
    "INPUT_FILE = \"crf_datafinalkdd22.pkl\"\n",
    "OUTPUT_FOLDER = \"hate_span_elmo_crf_weight\"\n",
    "OUTPUT_FILE = \"elmo_weights\"\n",
    "\n",
    "ELMO_MODEL = \"https://tfhub.dev/google/elmo/2\"\n",
    "MAX_LEN = 60\n",
    "TEST_SIZE = 448\n",
    "\n",
    "N_CRF_TAGS = 3\n",
    "LSTM_UNITS = 512\n",
    "DENSE_UNITS = 50\n",
    "LSTM_DROPOUT = 0.2\n",
    "DENSE_DROPOUT = 0.2\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base ELMO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model = hub.Module(ELMO_MODEL, trainable=True)\n",
    "\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "        \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "        \"sequence_len\": tf.constant(batch_size * [MAX_LEN])\n",
    "    },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un86HA3NKs5w"
   },
   "outputs": [],
   "source": [
    "crf = CRF(N_CRF_TAGS)\n",
    "input_text = keras.Input(shape=(None, ), dtype='string')\n",
    "embedding = keras.layers.Lambda(ElmoEmbedding,\n",
    "                                output_shape=(60, 1024))(input_text)\n",
    "x = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(units=LSTM_UNITS,\n",
    "                      return_sequences=True,\n",
    "                      recurrent_dropout=LSTM_DROPOUT,\n",
    "                      dropout=LSTM_DROPOUT))(embedding)\n",
    "x_rnn = keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(units=LSTM_UNITS,\n",
    "                      return_sequences=True,\n",
    "                      recurrent_dropout=LSTM_DROPOUT,\n",
    "                      dropout=LSTM_DROPOUT))(embedding)\n",
    "x = keras.layers.add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "base_model = keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(DENSE_UNITS, activation=\"relu\"))(x)\n",
    "out = crf(base_model)  # CRF layer\n",
    "model = keras.models.Model(input_text, out)\n",
    "model.compile(optimizer='adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GjHS8e6KhdM"
   },
   "outputs": [],
   "source": [
    "with open(BASE_FOLDER + INPUT_FILE, \"rb\") as f:\n",
    "    input_data = pickle.load(f)\n",
    "\n",
    "words = list(set(input_data[\"Word\"].values))\n",
    "words.append(\"__PAD__\")\n",
    "n_words = len(words)\n",
    "tags = list(set(input_data[\"Tag\"].values))\n",
    "n_tags = len(tags)\n",
    "assert n_tags == N_CRF_TAGS\n",
    "postags = list(set(input_data['POS'].values))\n",
    "postags.append(\"XX\")\n",
    "n_postags = len(postags)\n",
    "\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(\n",
    "            s[\"Word\"].values.tolist(), s[\"POS\"].values.tolist(), s[\"Tag\"].\n",
    "            values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "sentences = getter.sentences\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "postag2idx = {t: i for i, t in enumerate(postags)}\n",
    "X = [[w[0] for w in s] for s in sentences]\n",
    "\n",
    "new_X = []\n",
    "for seq in X:\n",
    "    new_seq = []\n",
    "    for i in range(MAX_LEN):\n",
    "        try:\n",
    "            new_seq.append(seq[i])\n",
    "        except:\n",
    "            new_seq.append(\"__PAD__\")\n",
    "    new_X.append(new_seq)\n",
    "X = new_X\n",
    "\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=MAX_LEN,\n",
    "                  sequences=y,\n",
    "                  padding=\"post\",\n",
    "                  value=tag2idx[\"O\"])\n",
    "y = [to_categorical(i, num_classes=N_CRF_TAGS) for i in y]\n",
    "\n",
    "ypos = [[postag2idx[w[1]] for w in s] for s in sentences]\n",
    "ypos = pad_sequences(maxlen=MAX_LEN,\n",
    "                     sequences=ypos,\n",
    "                     padding=\"post\",\n",
    "                     value=postag2idx[\"XX\"])\n",
    "\n",
    "X_tr, X_te, y_tr, y_te, ypos_tr, ypos_te = train_test_split(\n",
    "    X, y, ypos, test_size=TEST_SIZE, random_state=SEED)\n",
    "print(len(X_tr), len(X_te), len(ypos_te))\n",
    "\n",
    "X_tr, X_val = X_tr[:72 * batch_size], X_tr[-8 * batch_size:]\n",
    "y_tr, y_val = y_tr[:72 * batch_size], y_tr[-8 * batch_size:]\n",
    "X_tr = np.array(X_tr)\n",
    "X_te = np.array(X_te)\n",
    "y_tr = np.array(y_tr)\n",
    "y_te = np.array(y_te)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val = np.asarray(y_val)\n",
    "\n",
    "train_steps = len(y_tr) // batch_size\n",
    "val_steps = len(y_te) // batch_size\n",
    "\n",
    "print(X_tr.shape, y_tr.shape, train_setps)\n",
    "print(X_tr.shape, X_val.shape, X_te.shape)\n",
    "\n",
    "assert X_tr.shape[0] % 32 == 0 and X_val.shape[0] % 32 == 0 and X_te.shape[\n",
    "    0] % 32 == 0\n",
    "print(X_tr.shape[0] / 32, X_val.shape[0] / 32, X_te.shape[0] / 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkxN81tdKvmP"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_te)\n",
    "\n",
    "## Here we convert the normal BIO output to the form BIO-POSTAG form.\n",
    "## This is for consumption into the seqeval classification report\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "\n",
    "\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"__PAD__\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "\n",
    "def test2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(idx2tag[p].replace(\"__PAD__\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "\n",
    "pred_labels = pred2label(y_pred)\n",
    "test_labels = test2label(np.argmax(y_te, -1))\n",
    "\n",
    "posidx2tag = {i: w for w, i in postag2idx.items()}\n",
    "\n",
    "true_labels_pos = []\n",
    "pred_labels_pos = []\n",
    "for idx in range(len(test_labels)):\n",
    "    true = test_labels[idx]\n",
    "    pred = pred_labels[idx]\n",
    "    new_true = []\n",
    "    new_pred = []\n",
    "    for i, pt in enumerate(ypos_te[idx]):\n",
    "        pt = posidx2tag[pt]\n",
    "        new_true.append(true[i] + \"-\" + pt)\n",
    "        new_pred.append(pred[i] + \"-\" + pt)\n",
    "    true_labels_pos.append(new_true)\n",
    "    pred_labels_pos.append(new_pred)\n",
    "\n",
    "## IMP NOTE: This classification report is from the seqeval library and NOT SKLEARN\n",
    "\n",
    "print(\n",
    "    classification_report(true_labels_pos,\n",
    "                          pred_labels_pos,\n",
    "                          digits=N_CRF_ATGS,\n",
    "                          scheme=IOB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OUQK539qGnP"
   },
   "outputs": [],
   "source": [
    "model.save_weights(BASE_FOLDER + OUTPUT_FOLDER + OUTPUT_FILE)\n",
    "\n",
    "## Save the tag-id maps\n",
    "with open(BASE_FOLDER+\"posidx2tag_elmo\",\"w\") as f:\n",
    "    json.dump(posidx2tag)\n",
    "with open(BASE_FOLDER+\"idx2tag_elmo\",\"w\") as f:\n",
    "    json.dump(idx2tag)    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "span_pred_elmo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
